import torch
import librosa
import soundfile as sf
import tempfile
import os
import numpy as np
from faster_whisper import WhisperModel  # ğŸ‘ˆ æ›¿æ¢åŸç‰ˆ whisper
from nemo.collections.asr.models import SortformerEncLabelModel
from datetime import timedelta
from scipy.signal import butter, filtfilt

# === è·¯å¾„é…ç½® ===
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
# 1. Faster-Whisper æ¨¡å‹è·¯å¾„
WHISPER_MODEL_DIR = os.path.join(BASE_DIR, "models", "faster-whisper")
# 2. NeMo æ¨¡å‹è·¯å¾„
NEMO_MODEL_PATH = os.path.join(BASE_DIR, "models", "nemo", "diar_sortformer_4spk-v1.nemo")

def preprocess_audio_for_whisper(audio_input, target_sr=16000):
    """éŸ³é¢‘é¢„å¤„ç†"""
    print(f"=== é¢„å¤„ç†éŸ³é¢‘: {audio_input} ===")
    audio, sr = librosa.load(audio_input, sr=target_sr, mono=True)
    print(f"é¢„å¤„ç†åéŸ³é¢‘é•¿åº¦: {len(audio)/sr:.2f} ç§’")
    return audio, sr

def run_whisper_asr(audio_input, model_size="large-v3"):
    """
    ä½¿ç”¨ Faster-Whisper è¿è¡Œ ASR
    """
    print(f"=== è¿è¡Œ Faster-Whisper ASR (æœ¬åœ°æ¨¡å‹: {WHISPER_MODEL_DIR}) ===")
    
    # é¢„å¤„ç†éŸ³é¢‘
    audio, sr = preprocess_audio_for_whisper(audio_input, 16000)
    
    # Faster-Whisper æœ€å¥½æ¥å—æ–‡ä»¶è·¯å¾„
    temp_audio_file = tempfile.NamedTemporaryFile(suffix='.wav', delete=False)
    sf.write(temp_audio_file.name, audio, sr)
    
    try:
        # === æ ¸å¿ƒä¿®æ”¹: åŠ è½½ Faster-Whisper ===
        # compute_type="float16" æ˜¯ GPU åŠ é€Ÿçš„å…³é”®
        # å¦‚æœä½ çš„æ˜¾å¡æ˜¾å­˜å°äº 8G ä¸”æŠ¥é”™ï¼Œå¯ä»¥æ”¹ä¸º "int8"
        model = WhisperModel(WHISPER_MODEL_DIR, device="cuda", compute_type="float32")
        
        print("å¼€å§‹è½¬å½•...")
        # beam_size=5 æ˜¯é»˜è®¤æ¨è
        # vad_filter=True å¼€å¯é™éŸ³è¿‡æ»¤ï¼Œå¤§å¹…æå‡é€Ÿåº¦ï¼
        segments_generator, info = model.transcribe(
            temp_audio_file.name,
            beam_size=5,
            language="zh",
            vad_filter=False,
            word_timestamps=True,
            temperature=0,
            condition_on_previous_text=False,
            no_speech_threshold=0.6
          #  vad_parameters=dict(min_silence_duration_ms=500)
        )
        
        print(f"æ£€æµ‹è¯­è¨€: {info.language}, æ¦‚ç‡: {info.language_probability:.2f}")
        
        # === å…³é”®: æ ¼å¼è½¬æ¢ ===
        # Faster-Whisper è¿”å›çš„æ˜¯å¯¹è±¡ç”Ÿæˆå™¨ï¼Œæˆ‘ä»¬éœ€è¦å°†å…¶è½¬æ¢ä¸º
        # åŒ…å«å­—å…¸çš„åˆ—è¡¨ï¼Œä»¥å…¼å®¹åç»­çš„ assign_speakers_to_segments å‡½æ•°
        segments = []
        for segment in segments_generator:
            segments.append({
                "start": segment.start,
                "end": segment.end,
                "text": segment.text.strip()
            })
            
        print(f"æ£€æµ‹åˆ°å¥å­æ®µè½æ•°: {len(segments)}")
        
        # è¿”å›å­—å…¸ç»“æ„ï¼Œä¿æŒä¸åŸç‰ˆ whisper ä»£ç å…¼å®¹
        return {
            "segments": segments,
            "language": info.language
        }
        
    finally:
        if os.path.exists(temp_audio_file.name):
            os.unlink(temp_audio_file.name)

def run_sortformer_diarization(audio_input, model_path=NEMO_MODEL_PATH):
    """è¿è¡ŒSortformerè¯­éŸ³è§’è‰²åˆ†ç¦»"""
    print(f"=== è¿è¡ŒSortformerè¯­éŸ³è§’è‰²åˆ†ç¦» (æ¨¡å‹: {model_path}) ===")
    
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"NeMo æ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°: {model_path}")

    # ä½¿ç”¨ç›¸åŒçš„é¢„å¤„ç†æ–¹æ³•
    audio, sr = preprocess_audio_for_whisper(audio_input, 16000)
    
    # ä¿å­˜ä¸´æ—¶æ–‡ä»¶
    temp_file = tempfile.NamedTemporaryFile(suffix='.wav', delete=False)
    sf.write(temp_file.name, audio, 16000)
    
    try:
        # åŠ è½½æ¨¡å‹
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        print(f"ä½¿ç”¨è®¾å¤‡: {device}")
        
        diar_model = SortformerEncLabelModel.restore_from(
            restore_path=model_path, 
            map_location=device, 
            strict=False
        )
        diar_model.eval()
        
        # æ‰§è¡Œè§’è‰²åˆ†ç¦»
        with torch.no_grad():
            predicted_segments = diar_model.diarize(
                audio=temp_file.name,
                batch_size=1
            )
        
        # è§£æç»“æœ
        segments = []
        if len(predicted_segments) > 0:
            segment_strings = predicted_segments[0]
            for segment_str in segment_strings:
                parts = segment_str.split()
                if len(parts) == 3:
                    segments.append({
                        'start': float(parts[0]),
                        'end': float(parts[1]),
                        'speaker': parts[2]
                    })
        
        segments.sort(key=lambda x: x['start'])
        print(f"æ£€æµ‹åˆ°è¯´è¯äººæ®µè½æ•°: {len(segments)}")
        return segments
        
    finally:
        os.unlink(temp_file.name)

def assign_speakers_to_segments(whisper_result, diarization_segments):
    """ç»™Whisperçš„æ¯ä¸ªå¥å­æ®µè½åˆ†é…è¯´è¯äººæ ‡ç­¾"""
    print("=== ç»™Whisperå¥å­åˆ†é…è¯´è¯äººæ ‡ç­¾ ===")
    
    segments_with_speakers = []
    # æ³¨æ„ï¼šè¿™é‡Œ whisper_result['segments'] ç°åœ¨æ˜¯æˆ‘ä»¬æ‰‹åŠ¨æ„é€ çš„å­—å…¸åˆ—è¡¨
    # æ‰€ä»¥å¯ä»¥ç”¨ ['start'] è®¿é—®ï¼Œä»£ç æ— éœ€ä¿®æ”¹
    whisper_segments = whisper_result['segments']

    if not diarization_segments:
        print("âš ï¸ è­¦å‘Šï¼šæ²¡æœ‰æ£€æµ‹åˆ°è¯´è¯äººåˆ†æ®µï¼Œæ‰€æœ‰å­—å¹•æ ‡è®°ä¸º Speaker 0")
        for seg in whisper_segments:
            segments_with_speakers.append({**seg, 'speaker': 'speaker_0'})
        return segments_with_speakers
    
    for segment in whisper_segments:
        segment_start = segment['start']
        segment_end = segment['end']
        segment_center = (segment_start + segment_end) / 2
        
        # é»˜è®¤ç¬¬ä¸€ä¸ªè¯´è¯äºº
        best_speaker = diarization_segments[0]['speaker']
        max_overlap = 0.0
        
        for diar_seg in diarization_segments:
            overlap_start = max(segment_start, diar_seg['start'])
            overlap_end = min(segment_end, diar_seg['end'])
            overlap = max(0, overlap_end - overlap_start)
            
            if overlap > max_overlap:
                max_overlap = overlap
                best_speaker = diar_seg['speaker']
        
        if max_overlap == 0:
            min_distance = float('inf')
            for diar_seg in diarization_segments:
                distance = min(
                    abs(segment_center - diar_seg['start']),
                    abs(segment_center - diar_seg['end'])
                )
                if distance < min_distance:
                    min_distance = distance
                    best_speaker = diar_seg['speaker']
        
        segments_with_speakers.append({
            'start': segment_start,
            'end': segment_end,
            'text': segment['text'],
            'speaker': best_speaker
        })
    
    return segments_with_speakers

def format_time_srt(seconds):
    """å°†ç§’æ•°è½¬æ¢ä¸ºSRTæ—¶é—´æ ¼å¼"""
    td = timedelta(seconds=seconds)
    hours = int(td.total_seconds() // 3600)
    minutes = int((td.total_seconds() % 3600) // 60)
    seconds = td.total_seconds() % 60
    milliseconds = int((seconds % 1) * 1000)
    seconds = int(seconds)
    return f"{hours:02d}:{minutes:02d}:{seconds:02d},{milliseconds:03d}"

def create_srt_with_speakers(segments_with_speakers, output_file):
    """ç”Ÿæˆå¸¦æœ‰è¯´è¯äººæ ‡è®°çš„SRTå­—å¹•æ–‡ä»¶"""
    print(f"=== ç”ŸæˆSRTå­—å¹•: {output_file} ===")
    with open(output_file, 'w', encoding='utf-8') as f:
        for i, segment in enumerate(segments_with_speakers, 1):
            start_time = format_time_srt(segment['start'])
            end_time = format_time_srt(segment['end'])
            speaker_label = segment['speaker'].replace('speaker_', 'Speaker ')
            text_with_speaker = f"[{speaker_label}] {segment['text']}"
            
            f.write(f"{i}\n")
            f.write(f"{start_time} --> {end_time}\n")
            f.write(f"{text_with_speaker}\n\n")

def create_simple_srt(whisper_result, output_file):
    """ä¸ä½¿ç”¨è¯´è¯äººåˆ†ç¦»æ—¶ï¼Œç”Ÿæˆç®€å•çš„SRTå­—å¹•"""
    print(f"=== ç”Ÿæˆç®€å•SRTå­—å¹•: {output_file} ===")
    with open(output_file, 'w', encoding='utf-8') as f:
        # æ³¨æ„ï¼šè¿™é‡Œ whisper_result['segments'] ä¹Ÿæ˜¯æˆ‘ä»¬æ„é€ çš„å­—å…¸åˆ—è¡¨
        for i, segment in enumerate(whisper_result['segments'], 1):
            start_time = format_time_srt(segment['start'])
            end_time = format_time_srt(segment['end'])
            text = segment['text']

            f.write(f"{i}\n")
            f.write(f"{start_time} --> {end_time}\n")
            f.write(f"{text}\n\n")

def whisper_with_diarization(audio_input, 
                           whisper_model="large-v3",
                           diar_model_path=NEMO_MODEL_PATH, # ä½¿ç”¨æ–°å¸¸é‡
                           output_srt="output_with_speakers.srt"):
    """å®Œæ•´çš„æµç¨‹"""
    print(f"å¤„ç†éŸ³é¢‘æ–‡ä»¶: {audio_input}")
    
    # 1. è¿è¡Œ Faster-Whisper ASR
    whisper_result = run_whisper_asr(audio_input, whisper_model)
    
    # 2. è¿è¡Œè¯­éŸ³è§’è‰²åˆ†ç¦»
    # æ³¨æ„ï¼šè¿™é‡Œ diar_model_path é»˜è®¤å€¼å·²ç»æ˜¯æ–°çš„æœ¬åœ°è·¯å¾„äº†
    diarization_segments = run_sortformer_diarization(audio_input, diar_model_path)
    
    # 3. ç»™Whisperå¥å­åˆ†é…è¯´è¯äººæ ‡ç­¾
    segments_with_speakers = assign_speakers_to_segments(whisper_result, diarization_segments)
    
    # 4. ç”ŸæˆSRTå­—å¹•
    create_srt_with_speakers(segments_with_speakers, output_srt)
    
    return segments_with_speakers

if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    pass
