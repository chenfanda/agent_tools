{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e0e8139-915e-4b93-9c1a-3105e402f6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.messages import TextMessage\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_core import CancellationToken\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69b998a6-8ff1-4db2-afba-5915973f3d07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mAssistantAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmodel_client\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mautogen_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mChatCompletionClient\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtools\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mautogen_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseTool\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAwaitable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mhandoffs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mautogen_agentchat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handoff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHandoff\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmodel_context\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mautogen_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_chat_completion_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mChatCompletionContext\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdescription\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'An agent that provides assistance with ability to use tools.'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msystem_message\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'You are a helpful AI assistant. Solve tasks using your tools. Reply with TERMINATE when the task has been completed.'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmodel_client_stream\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreflect_on_tool_use\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtool_call_summary_format\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'{result}'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmemory\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mautogen_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_base_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMemory\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "An agent that provides assistance with tool use.\n",
       "\n",
       "The :meth:`on_messages` returns a :class:`~autogen_agentchat.base.Response`\n",
       "in which :attr:`~autogen_agentchat.base.Response.chat_message` is the final\n",
       "response message.\n",
       "\n",
       "The :meth:`on_messages_stream` creates an async generator that produces\n",
       "the inner messages as they are created, and the :class:`~autogen_agentchat.base.Response`\n",
       "object as the last item before closing the generator.\n",
       "\n",
       ".. attention::\n",
       "\n",
       "    The caller must only pass the new messages to the agent on each call\n",
       "    to the :meth:`on_messages` or :meth:`on_messages_stream` method.\n",
       "    The agent maintains its state between calls to these methods.\n",
       "    Do not pass the entire conversation history to the agent on each call.\n",
       "\n",
       ".. warning::\n",
       "    The assistant agent is not thread-safe or coroutine-safe.\n",
       "    It should not be shared between multiple tasks or coroutines, and it should\n",
       "    not call its methods concurrently.\n",
       "\n",
       "The following diagram shows how the assistant agent works:\n",
       "\n",
       ".. image:: ../../images/assistant-agent.svg\n",
       "\n",
       "Tool call behavior:\n",
       "\n",
       "* If the model returns no tool call, then the response is immediately returned as a :class:`~autogen_agentchat.messages.TextMessage` in :attr:`~autogen_agentchat.base.Response.chat_message`.\n",
       "* When the model returns tool calls, they will be executed right away:\n",
       "    - When `reflect_on_tool_use` is False (default), the tool call results are returned as a :class:`~autogen_agentchat.messages.ToolCallSummaryMessage` in :attr:`~autogen_agentchat.base.Response.chat_message`. `tool_call_summary_format` can be used to customize the tool call summary.\n",
       "    - When `reflect_on_tool_use` is True, the another model inference is made using the tool calls and results, and the text response is returned as a :class:`~autogen_agentchat.messages.TextMessage` in :attr:`~autogen_agentchat.base.Response.chat_message`.\n",
       "* If the model returns multiple tool calls, they will be executed concurrently. To disable parallel tool calls you need to configure the model client. For example, set `parallel_tool_calls=False` for :class:`~autogen_ext.models.openai.OpenAIChatCompletionClient` and :class:`~autogen_ext.models.openai.AzureOpenAIChatCompletionClient`.\n",
       "\n",
       ".. tip::\n",
       "    By default, the tool call results are returned as response when tool calls are made.\n",
       "    So it is recommended to pay attention to the formatting of the tools return values,\n",
       "    especially if another agent is expecting them in a specific format.\n",
       "    Use `tool_call_summary_format` to customize the tool call summary, if needed.\n",
       "\n",
       "Hand off behavior:\n",
       "\n",
       "* If a handoff is triggered, a :class:`~autogen_agentchat.messages.HandoffMessage` will be returned in :attr:`~autogen_agentchat.base.Response.chat_message`.\n",
       "* If there are tool calls, they will also be executed right away before returning the handoff.\n",
       "* The tool calls and results are passed to the target agent through :attr:`~autogen_agentchat.messages.HandoffMessage.context`.\n",
       "\n",
       "\n",
       ".. note::\n",
       "    If multiple handoffs are detected, only the first handoff is executed.\n",
       "    To avoid this, disable parallel tool calls in the model client configuration.\n",
       "\n",
       "\n",
       "Limit context size sent to the model:\n",
       "\n",
       "You can limit the number of messages sent to the model by setting\n",
       "the `model_context` parameter to a :class:`~autogen_core.model_context.BufferedChatCompletionContext`.\n",
       "This will limit the number of recent messages sent to the model and can be useful\n",
       "when the model has a limit on the number of tokens it can process.\n",
       "You can also create your own model context by subclassing\n",
       ":class:`~autogen_core.model_context.ChatCompletionContext`.\n",
       "\n",
       "Streaming mode:\n",
       "\n",
       "The assistant agent can be used in streaming mode by setting `model_client_stream=True`.\n",
       "In this mode, the :meth:`on_messages_stream` and :meth:`BaseChatAgent.run_stream` methods will also yield\n",
       ":class:`~autogen_agentchat.messages.ModelClientStreamingChunkEvent`\n",
       "messages as the model client produces chunks of response.\n",
       "The chunk messages will not be included in the final response's inner messages.\n",
       "\n",
       "\n",
       "Args:\n",
       "    name (str): The name of the agent.\n",
       "    model_client (ChatCompletionClient): The model client to use for inference.\n",
       "    tools (List[BaseTool[Any, Any]  | Callable[..., Any] | Callable[..., Awaitable[Any]]] | None, optional): The tools to register with the agent.\n",
       "    handoffs (List[HandoffBase | str] | None, optional): The handoff configurations for the agent,\n",
       "        allowing it to transfer to other agents by responding with a :class:`HandoffMessage`.\n",
       "        The transfer is only executed when the team is in :class:`~autogen_agentchat.teams.Swarm`.\n",
       "        If a handoff is a string, it should represent the target agent's name.\n",
       "    model_context (ChatCompletionContext | None, optional): The model context for storing and retrieving :class:`~autogen_core.models.LLMMessage`. It can be preloaded with initial messages. The initial messages will be cleared when the agent is reset.\n",
       "    description (str, optional): The description of the agent.\n",
       "    system_message (str, optional): The system message for the model. If provided, it will be prepended to the messages in the model context when making an inference. Set to `None` to disable.\n",
       "    model_client_stream (bool, optional): If `True`, the model client will be used in streaming mode.\n",
       "        :meth:`on_messages_stream` and :meth:`BaseChatAgent.run_stream` methods will also yield :class:`~autogen_agentchat.messages.ModelClientStreamingChunkEvent`\n",
       "        messages as the model client produces chunks of response. Defaults to `False`.\n",
       "    reflect_on_tool_use (bool, optional): If `True`, the agent will make another model inference using the tool call and result\n",
       "        to generate a response. If `False`, the tool call result will be returned as the response. Defaults to `False`.\n",
       "    tool_call_summary_format (str, optional): The format string used to create a tool call summary for every tool call result.\n",
       "        Defaults to \"{result}\".\n",
       "        When `reflect_on_tool_use` is `False`, a concatenation of all the tool call summaries, separated by a new line character ('\\n')\n",
       "        will be returned as the response.\n",
       "        Available variables: `{tool_name}`, `{arguments}`, `{result}`.\n",
       "        For example, `\"{tool_name}: {result}\"` will create a summary like `\"tool_name: result\"`.\n",
       "    memory (Sequence[Memory] | None, optional): The memory store to use for the agent. Defaults to `None`.\n",
       "\n",
       "Raises:\n",
       "    ValueError: If tool names are not unique.\n",
       "    ValueError: If handoff names are not unique.\n",
       "    ValueError: If handoff names are not unique from tool names.\n",
       "    ValueError: If maximum number of tool iterations is less than 1.\n",
       "\n",
       "Examples:\n",
       "\n",
       "    **Example 1: basic agent**\n",
       "\n",
       "    The following example demonstrates how to create an assistant agent with\n",
       "    a model client and generate a response to a simple task.\n",
       "\n",
       "    .. code-block:: python\n",
       "\n",
       "        import asyncio\n",
       "        from autogen_core import CancellationToken\n",
       "        from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
       "        from autogen_agentchat.agents import AssistantAgent\n",
       "        from autogen_agentchat.messages import TextMessage\n",
       "\n",
       "\n",
       "        async def main() -> None:\n",
       "            model_client = OpenAIChatCompletionClient(\n",
       "                model=\"gpt-4o\",\n",
       "                # api_key = \"your_openai_api_key\"\n",
       "            )\n",
       "            agent = AssistantAgent(name=\"assistant\", model_client=model_client)\n",
       "\n",
       "            response = await agent.on_messages(\n",
       "                [TextMessage(content=\"What is the capital of France?\", source=\"user\")], CancellationToken()\n",
       "            )\n",
       "            print(response)\n",
       "\n",
       "\n",
       "        asyncio.run(main())\n",
       "\n",
       "    **Example 2: model client token streaming**\n",
       "\n",
       "    This example demonstrates how to create an assistant agent with\n",
       "    a model client and generate a token stream by setting `model_client_stream=True`.\n",
       "\n",
       "    .. code-block:: python\n",
       "\n",
       "        import asyncio\n",
       "        from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
       "        from autogen_agentchat.agents import AssistantAgent\n",
       "        from autogen_agentchat.messages import TextMessage\n",
       "        from autogen_core import CancellationToken\n",
       "\n",
       "\n",
       "        async def main() -> None:\n",
       "            model_client = OpenAIChatCompletionClient(\n",
       "                model=\"gpt-4o\",\n",
       "                # api_key = \"your_openai_api_key\"\n",
       "            )\n",
       "            agent = AssistantAgent(\n",
       "                name=\"assistant\",\n",
       "                model_client=model_client,\n",
       "                model_client_stream=True,\n",
       "            )\n",
       "\n",
       "            stream = agent.on_messages_stream(\n",
       "                [TextMessage(content=\"Name two cities in North America.\", source=\"user\")], CancellationToken()\n",
       "            )\n",
       "            async for message in stream:\n",
       "                print(message)\n",
       "\n",
       "\n",
       "        asyncio.run(main())\n",
       "\n",
       "    .. code-block:: text\n",
       "\n",
       "        source='assistant' models_usage=None content='Two' type='ModelClientStreamingChunkEvent'\n",
       "        source='assistant' models_usage=None content=' cities' type='ModelClientStreamingChunkEvent'\n",
       "        source='assistant' models_usage=None content=' in' type='ModelClientStreamingChunkEvent'\n",
       "        source='assistant' models_usage=None content=' North' type='ModelClientStreamingChunkEvent'\n",
       "        source='assistant' models_usage=None content=' America' type='ModelClientStreamingChunkEvent'\n",
       "        source='assistant' models_usage=None content=' are' type='ModelClientStreamingChunkEvent'\n",
       "        source='assistant' models_usage=None content=' New' type='ModelClientStreamingChunkEvent'\n",
       "        source='assistant' models_usage=None content=' York' type='ModelClientStreamingChunkEvent'\n",
       "        source='assistant' models_usage=None content=' City' type='ModelClientStreamingChunkEvent'\n",
       "        source='assistant' models_usage=None content=' in' type='ModelClientStreamingChunkEvent'\n",
       "        source='assistant' models_usage=None content=' the' type='ModelClientStreamingChunkEvent'\n",
       "        source='assistant' models_usage=None content=' United' type='ModelClientStreamingChunkEvent'\n",
       "        source='assistant' models_usage=None content=' States' type='ModelClientStreamingChunkEvent'\n",
       "        source='assistant' models_usage=None content=' and' type='ModelClientStreamingChunkEvent'\n",
       "        source='assistant' models_usage=None content=' Toronto' type='ModelClientStreamingChunkEvent'\n",
       "        source='assistant' models_usage=None content=' in' type='ModelClientStreamingChunkEvent'\n",
       "        source='assistant' models_usage=None content=' Canada' type='ModelClientStreamingChunkEvent'\n",
       "        source='assistant' models_usage=None content='.' type='ModelClientStreamingChunkEvent'\n",
       "        source='assistant' models_usage=None content=' TERMIN' type='ModelClientStreamingChunkEvent'\n",
       "        source='assistant' models_usage=None content='ATE' type='ModelClientStreamingChunkEvent'\n",
       "        Response(chat_message=TextMessage(source='assistant', models_usage=RequestUsage(prompt_tokens=0, completion_tokens=0), content='Two cities in North America are New York City in the United States and Toronto in Canada. TERMINATE', type='TextMessage'), inner_messages=[])\n",
       "\n",
       "\n",
       "    **Example 3: agent with tools**\n",
       "\n",
       "    The following example demonstrates how to create an assistant agent with\n",
       "    a model client and a tool, generate a stream of messages for a task, and\n",
       "    print the messages to the console using :class:`~autogen_agentchat.ui.Console`.\n",
       "\n",
       "    The tool is a simple function that returns the current time.\n",
       "    Under the hood, the function is wrapped in a :class:`~autogen_core.tools.FunctionTool`\n",
       "    and used with the agent's model client. The doc string of the function\n",
       "    is used as the tool description, the function name is used as the tool name,\n",
       "    and the function signature including the type hints is used as the tool arguments.\n",
       "\n",
       "    .. code-block:: python\n",
       "\n",
       "        import asyncio\n",
       "        from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
       "        from autogen_agentchat.agents import AssistantAgent\n",
       "        from autogen_agentchat.messages import TextMessage\n",
       "        from autogen_agentchat.ui import Console\n",
       "        from autogen_core import CancellationToken\n",
       "\n",
       "\n",
       "        async def get_current_time() -> str:\n",
       "            return \"The current time is 12:00 PM.\"\n",
       "\n",
       "\n",
       "        async def main() -> None:\n",
       "            model_client = OpenAIChatCompletionClient(\n",
       "                model=\"gpt-4o\",\n",
       "                # api_key = \"your_openai_api_key\"\n",
       "            )\n",
       "            agent = AssistantAgent(name=\"assistant\", model_client=model_client, tools=[get_current_time])\n",
       "\n",
       "            await Console(\n",
       "                agent.on_messages_stream(\n",
       "                    [TextMessage(content=\"What is the current time?\", source=\"user\")], CancellationToken()\n",
       "                )\n",
       "            )\n",
       "\n",
       "\n",
       "        asyncio.run(main())\n",
       "\n",
       "    **Example 4: agent with structured output and tool**\n",
       "\n",
       "    The following example demonstrates how to create an assistant agent with\n",
       "    a model client configured to use structured output and a tool.\n",
       "    Note that you need to use :class:`~autogen_core.tools.FunctionTool` to create the tool\n",
       "    and the `strict=True` is required for structured output mode.\n",
       "    Because the model is configured to use structured output, the output\n",
       "    reflection response will be a JSON formatted string.\n",
       "\n",
       "    .. code-block:: python\n",
       "\n",
       "        import asyncio\n",
       "        from typing import Literal\n",
       "\n",
       "        from autogen_agentchat.agents import AssistantAgent\n",
       "        from autogen_agentchat.messages import TextMessage\n",
       "        from autogen_agentchat.ui import Console\n",
       "        from autogen_core import CancellationToken\n",
       "        from autogen_core.tools import FunctionTool\n",
       "        from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
       "        from pydantic import BaseModel\n",
       "\n",
       "\n",
       "        # Define the structured output format.\n",
       "        class AgentResponse(BaseModel):\n",
       "            thoughts: str\n",
       "            response: Literal[\"happy\", \"sad\", \"neutral\"]\n",
       "\n",
       "\n",
       "        # Define the function to be called as a tool.\n",
       "        def sentiment_analysis(text: str) -> str:\n",
       "            \"\"\"Given a text, return the sentiment.\"\"\"\n",
       "            return \"happy\" if \"happy\" in text else \"sad\" if \"sad\" in text else \"neutral\"\n",
       "\n",
       "\n",
       "        # Create a FunctionTool instance with `strict=True`,\n",
       "        # which is required for structured output mode.\n",
       "        tool = FunctionTool(sentiment_analysis, description=\"Sentiment Analysis\", strict=True)\n",
       "\n",
       "        # Create an OpenAIChatCompletionClient instance that uses the structured output format.\n",
       "        model_client = OpenAIChatCompletionClient(\n",
       "            model=\"gpt-4o-mini\",\n",
       "            response_format=AgentResponse,  # type: ignore\n",
       "        )\n",
       "\n",
       "        # Create an AssistantAgent instance that uses the tool and model client.\n",
       "        agent = AssistantAgent(\n",
       "            name=\"assistant\",\n",
       "            model_client=model_client,\n",
       "            tools=[tool],\n",
       "            system_message=\"Use the tool to analyze sentiment.\",\n",
       "            reflect_on_tool_use=True,  # Use reflection to have the agent generate a formatted response.\n",
       "        )\n",
       "\n",
       "\n",
       "        async def main() -> None:\n",
       "            stream = agent.on_messages_stream([TextMessage(content=\"I am happy today!\", source=\"user\")], CancellationToken())\n",
       "            await Console(stream)\n",
       "\n",
       "\n",
       "        asyncio.run(main())\n",
       "\n",
       "    .. code-block:: text\n",
       "\n",
       "        ---------- assistant ----------\n",
       "        [FunctionCall(id='call_tIZjAVyKEDuijbBwLY6RHV2p', arguments='{\"text\":\"I am happy today!\"}', name='sentiment_analysis')]\n",
       "        ---------- assistant ----------\n",
       "        [FunctionExecutionResult(content='happy', call_id='call_tIZjAVyKEDuijbBwLY6RHV2p', is_error=False)]\n",
       "        ---------- assistant ----------\n",
       "        {\"thoughts\":\"The user expresses a clear positive emotion by stating they are happy today, suggesting an upbeat mood.\",\"response\":\"happy\"}\n",
       "\n",
       "    **Example 5: agent with bounded model context**\n",
       "\n",
       "    The following example shows how to use a\n",
       "    :class:`~autogen_core.model_context.BufferedChatCompletionContext`\n",
       "    that only keeps the last 2 messages (1 user + 1 assistant).\n",
       "    Bounded model context is useful when the model has a limit on the\n",
       "    number of tokens it can process.\n",
       "\n",
       "    .. code-block:: python\n",
       "\n",
       "        import asyncio\n",
       "\n",
       "        from autogen_agentchat.agents import AssistantAgent\n",
       "        from autogen_agentchat.messages import TextMessage\n",
       "        from autogen_core import CancellationToken\n",
       "        from autogen_core.model_context import BufferedChatCompletionContext\n",
       "        from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
       "\n",
       "\n",
       "        async def main() -> None:\n",
       "            # Create a model client.\n",
       "            model_client = OpenAIChatCompletionClient(\n",
       "                model=\"gpt-4o-mini\",\n",
       "                # api_key = \"your_openai_api_key\"\n",
       "            )\n",
       "\n",
       "            # Create a model context that only keeps the last 2 messages (1 user + 1 assistant).\n",
       "            model_context = BufferedChatCompletionContext(buffer_size=2)\n",
       "\n",
       "            # Create an AssistantAgent instance with the model client and context.\n",
       "            agent = AssistantAgent(\n",
       "                name=\"assistant\",\n",
       "                model_client=model_client,\n",
       "                model_context=model_context,\n",
       "                system_message=\"You are a helpful assistant.\",\n",
       "            )\n",
       "\n",
       "            response = await agent.on_messages(\n",
       "                [TextMessage(content=\"Name two cities in North America.\", source=\"user\")], CancellationToken()\n",
       "            )\n",
       "            print(response.chat_message.content)  # type: ignore\n",
       "\n",
       "            response = await agent.on_messages(\n",
       "                [TextMessage(content=\"My favorite color is blue.\", source=\"user\")], CancellationToken()\n",
       "            )\n",
       "            print(response.chat_message.content)  # type: ignore\n",
       "\n",
       "            response = await agent.on_messages(\n",
       "                [TextMessage(content=\"Did I ask you any question?\", source=\"user\")], CancellationToken()\n",
       "            )\n",
       "            print(response.chat_message.content)  # type: ignore\n",
       "\n",
       "\n",
       "        asyncio.run(main())\n",
       "\n",
       "    .. code-block:: text\n",
       "\n",
       "        Two cities in North America are New York City and Toronto.\n",
       "        That's great! Blue is often associated with calmness and serenity. Do you have a specific shade of blue that you like, or any particular reason why it's your favorite?\n",
       "        No, you didn't ask a question. I apologize for any misunderstanding. If you have something specific you'd like to discuss or ask, feel free to let me know!\n",
       "\n",
       "    **Example 6: agent with memory**\n",
       "\n",
       "    The following example shows how to use a list-based memory with the assistant agent.\n",
       "    The memory is preloaded with some initial content.\n",
       "    Under the hood, the memory is used to update the model context\n",
       "    before making an inference, using the :meth:`~autogen_core.memory.Memory.update_context` method.\n",
       "\n",
       "    .. code-block:: python\n",
       "\n",
       "        import asyncio\n",
       "\n",
       "        from autogen_agentchat.agents import AssistantAgent\n",
       "        from autogen_agentchat.messages import TextMessage\n",
       "        from autogen_core import CancellationToken\n",
       "        from autogen_core.memory import ListMemory, MemoryContent\n",
       "        from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
       "\n",
       "\n",
       "        async def main() -> None:\n",
       "            # Create a model client.\n",
       "            model_client = OpenAIChatCompletionClient(\n",
       "                model=\"gpt-4o-mini\",\n",
       "                # api_key = \"your_openai_api_key\"\n",
       "            )\n",
       "\n",
       "            # Create a list-based memory with some initial content.\n",
       "            memory = ListMemory()\n",
       "            await memory.add(MemoryContent(content=\"User likes pizza.\", mime_type=\"text/plain\"))\n",
       "            await memory.add(MemoryContent(content=\"User dislikes cheese.\", mime_type=\"text/plain\"))\n",
       "\n",
       "            # Create an AssistantAgent instance with the model client and memory.\n",
       "            agent = AssistantAgent(\n",
       "                name=\"assistant\",\n",
       "                model_client=model_client,\n",
       "                memory=[memory],\n",
       "                system_message=\"You are a helpful assistant.\",\n",
       "            )\n",
       "\n",
       "            response = await agent.on_messages(\n",
       "                [TextMessage(content=\"One idea for a dinner.\", source=\"user\")], CancellationToken()\n",
       "            )\n",
       "            print(response.chat_message.content)  # type: ignore\n",
       "\n",
       "\n",
       "        asyncio.run(main())\n",
       "\n",
       "    .. code-block:: text\n",
       "\n",
       "        How about making a delicious pizza without cheese? You can create a flavorful veggie pizza with a variety of toppings. Here's a quick idea:\n",
       "\n",
       "        **Veggie Tomato Sauce Pizza**\n",
       "        - Start with a pizza crust (store-bought or homemade).\n",
       "        - Spread a layer of marinara or tomato sauce evenly over the crust.\n",
       "        - Top with your favorite vegetables like bell peppers, mushrooms, onions, olives, and spinach.\n",
       "        - Add some protein if you’d like, such as grilled chicken or pepperoni (ensure it's cheese-free).\n",
       "        - Sprinkle with herbs like oregano and basil, and maybe a drizzle of olive oil.\n",
       "        - Bake according to the crust instructions until the edges are golden and the veggies are cooked.\n",
       "\n",
       "        Serve it with a side salad or some garlic bread to complete the meal! Enjoy your dinner!\n",
       "\n",
       "    **Example 7: agent with `o1-mini`**\n",
       "\n",
       "    The following example shows how to use `o1-mini` model with the assistant agent.\n",
       "\n",
       "    .. code-block:: python\n",
       "\n",
       "        import asyncio\n",
       "        from autogen_core import CancellationToken\n",
       "        from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
       "        from autogen_agentchat.agents import AssistantAgent\n",
       "        from autogen_agentchat.messages import TextMessage\n",
       "\n",
       "\n",
       "        async def main() -> None:\n",
       "            model_client = OpenAIChatCompletionClient(\n",
       "                model=\"o1-mini\",\n",
       "                # api_key = \"your_openai_api_key\"\n",
       "            )\n",
       "            # The system message is not supported by the o1 series model.\n",
       "            agent = AssistantAgent(name=\"assistant\", model_client=model_client, system_message=None)\n",
       "\n",
       "            response = await agent.on_messages(\n",
       "                [TextMessage(content=\"What is the capital of France?\", source=\"user\")], CancellationToken()\n",
       "            )\n",
       "            print(response)\n",
       "\n",
       "\n",
       "        asyncio.run(main())\n",
       "\n",
       "    .. note::\n",
       "\n",
       "        The `o1-preview` and `o1-mini` models do not support system message and function calling.\n",
       "        So the `system_message` should be set to `None` and the `tools` and `handoffs` should not be set.\n",
       "        See `o1 beta limitations <https://platform.openai.com/docs/guides/reasoning#beta-limitations>`_ for more details.\n",
       "\n",
       "\n",
       "    **Example 8: agent using reasoning model with custom model context.**\n",
       "\n",
       "    The following example shows how to use a reasoning model (DeepSeek R1) with the assistant agent.\n",
       "    The model context is used to filter out the thought field from the assistant message.\n",
       "\n",
       "    .. code-block:: python\n",
       "\n",
       "        import asyncio\n",
       "        from typing import List\n",
       "\n",
       "        from autogen_agentchat.agents import AssistantAgent\n",
       "        from autogen_core.model_context import UnboundedChatCompletionContext\n",
       "        from autogen_core.models import AssistantMessage, LLMMessage, ModelFamily\n",
       "        from autogen_ext.models.ollama import OllamaChatCompletionClient\n",
       "\n",
       "\n",
       "        class ReasoningModelContext(UnboundedChatCompletionContext):\n",
       "            \"\"\"A model context for reasoning models.\"\"\"\n",
       "\n",
       "            async def get_messages(self) -> List[LLMMessage]:\n",
       "                messages = await super().get_messages()\n",
       "                # Filter out thought field from AssistantMessage.\n",
       "                messages_out: List[LLMMessage] = []\n",
       "                for message in messages:\n",
       "                    if isinstance(message, AssistantMessage):\n",
       "                        message.thought = None\n",
       "                    messages_out.append(message)\n",
       "                return messages_out\n",
       "\n",
       "\n",
       "        # Create an instance of the model client for DeepSeek R1 hosted locally on Ollama.\n",
       "        model_client = OllamaChatCompletionClient(\n",
       "            model=\"deepseek-r1:8b\",\n",
       "            model_info={\n",
       "                \"vision\": False,\n",
       "                \"function_calling\": False,\n",
       "                \"json_output\": False,\n",
       "                \"family\": ModelFamily.R1,\n",
       "            },\n",
       "        )\n",
       "\n",
       "        agent = AssistantAgent(\n",
       "            \"reasoning_agent\",\n",
       "            model_client=model_client,\n",
       "            model_context=ReasoningModelContext(),  # Use the custom model context.\n",
       "        )\n",
       "\n",
       "\n",
       "        async def run_reasoning_agent() -> None:\n",
       "            result = await agent.run(task=\"What is the capital of France?\")\n",
       "            print(result)\n",
       "\n",
       "\n",
       "        asyncio.run(run_reasoning_agent())\n",
       "\u001b[0;31mFile:\u001b[0m           /oper/ch/env/autogen_v4/lib/python3.10/site-packages/autogen_agentchat/agents/_assistant_agent.py\n",
       "\u001b[0;31mType:\u001b[0m           _ProtocolMeta\n",
       "\u001b[0;31mSubclasses:\u001b[0m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "AssistantAgent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41ee6941-0d7c-43a8-a271-1fce3bcb7401",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import httpx\n",
    "import logging\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "#wechat search and send"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "92aa9759-e124-4d00-a1f1-cee6b0fc2561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'status': 'success', 'message': '成功向联系人 陈浩 发送消息', 'contact_name': '陈浩', 'contact_id': ''}\n"
     ]
    }
   ],
   "source": [
    "processed_params= {\"contact_name\": \"陈浩\",\n",
    "                    \"message\": \"测试微信数据接口 \"\n",
    "}\n",
    "async with httpx.AsyncClient() as client:\n",
    "    tool_response= await client.post(\n",
    "    \"http://localhost:8003/tools/wechat/search_and_send\",\n",
    "    json=processed_params,\n",
    "    timeout=300.0\n",
    "    )\n",
    "tool_response.raise_for_status()\n",
    "print(tool_response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0a29fa-1b9f-4ebc-9901-9b286866e535",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f0fab8f9-c1a6-422a-8c60-a22c47fe77c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a tool that searches the web for information.\n",
    "# async def citicbank_card_info(name:str) -> str:\n",
    "#     \"\"\"Find information on the citicbank web\"\"\"\n",
    "#     message=f'查询{name}的帐单。'\n",
    "#     return message\n",
    "# async def wechat_send(name:str,message: str) -> str:\n",
    "#     \"\"\"Send information by wechat\"\"\"\n",
    "#     processed_params= {\"contact_name\": name,\n",
    "#                     \"message\": message}\n",
    "#     async with httpx.AsyncClient() as client:\n",
    "#         tool_response= await client.post(\n",
    "#         \"http://localhost:8003/tools/wechat/search_and_send\",\n",
    "#         json=processed_params,\n",
    "#         timeout=300.0\n",
    "#         )\n",
    "#     return tool_response.json()\n",
    "\n",
    "async def wechat_send(name:str,message: str) -> str:\n",
    "    \"\"\"Send information by wechat\"\"\"\n",
    "\n",
    "    return f'send {message} to name !'\n",
    "\n",
    "# Create an agent that uses the OpenAI GPT-4o model.\n",
    "# model_client = OpenAIChatCompletionClient(\n",
    "#     model=\"gpt-4o\",\n",
    "#     # api_key=\"YOUR_API_KEY\",\n",
    "# )\n",
    "model_client = OpenAIChatCompletionClient(\n",
    "    model=\"gemma3:27b-tools\",\n",
    "    base_url=\"http://localhost:11434/v1\",\n",
    "    api_key=\"placeholder\",\n",
    "    model_info={\n",
    "        \"vision\": False,\n",
    "        \"function_calling\": True,\n",
    "        \"json_output\": False,\n",
    "        \"family\": \"unknown\",\n",
    "    },\n",
    ")\n",
    "agent = AssistantAgent(\n",
    "    name=\"assistant\",\n",
    "    model_client=model_client,\n",
    "    tools=[wechat_send],\n",
    "    system_message=\"当对话中提到发送信息时,使用工具完成任务，微信联系人提取对话文中的人名.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26569c7a-79ca-4c87-aa76-9b4de2d85421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# await Console(agent.run_stream(task='写个冒泡排序使用工具微信发给赵子豪'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e8f8baea-37f5-4e75-af19-63a684ad1f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# await Console(agent.run_stream(task='写一个agnet工具的创业计划，字数不少于300字，完成后使用微信发给联系人：陈浩'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "73183171-4fa2-4655-8106-fcf73bd21e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# async def assistant_run() -> None:\n",
    "#     response = await agent.on_messages(\n",
    "#         [TextMessage(content=\"写一个冒泡排序的代码。写完全部代码后再发陈浩。\", source=\"user\")],\n",
    "#         cancellation_token=CancellationToken(),\n",
    "#     )\n",
    "#     print(response.inner_messages)\n",
    "#     print(response.chat_message)\n",
    "\n",
    "\n",
    "# # Use asyncio.run(assistant_run()) when running in a script.\n",
    "# await assistant_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b51d094-c7c5-427f-acff-24ad44385368",
   "metadata": {},
   "outputs": [],
   "source": [
    "await Console(team.run_stream(task=task))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f50c5e-25d4-44b4-b2d7-96345143f89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "from autogen_agentchat.agents import AssistantAgent, UserProxyAgent\n",
    "from autogen_agentchat.conditions import MaxMessageTermination, TextMentionTermination\n",
    "from autogen_agentchat.messages import AgentEvent, ChatMessage\n",
    "from autogen_agentchat.teams import SelectorGroupChat\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4001b37-4cec-4a43-8e73-2b2c957654b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "planning_agent = AssistantAgent(\n",
    "    \"PlanningAgent\",\n",
    "    description=\"An agent for planning tasks, this agent should be the first to engage when given a new task.\",\n",
    "    model_client=model_client,\n",
    "    system_message=\"\"\"\n",
    "    You are a planning agent.\n",
    "    Your job is to break down complex tasks into smaller, manageable subtasks.\n",
    "    Your team members are:\n",
    "        WebSearchAgent: Searches for information\n",
    "        WechatAgent: send message by wechat\n",
    "\n",
    "    You only plan and delegate tasks - you do not execute them yourself.\n",
    "\n",
    "    When assigning tasks, use this format:\n",
    "    1. <agent> : <task>\n",
    "\n",
    "    After all tasks are complete, summarize the findings and end with \"TERMINATE\".\n",
    "    \"\"\",\n",
    ")\n",
    "\n",
    "web_search_agent = AssistantAgent(\n",
    "    \"WebSearchAgent\",\n",
    "    description=\"An agent for searching information on the web.\",\n",
    "    tools=[citicbank_card_info],\n",
    "    model_client=model_client,\n",
    "    system_message=\"\"\"\n",
    "    You are a web search agent.\n",
    "    Your only tool is search_tool - use it to find information.\n",
    "    You make only one search call at a time.\n",
    "    Once you have the results, you never do calculations based on them.\n",
    "    \"\"\",\n",
    ")\n",
    "\n",
    "wechat_send_agent = AssistantAgent(\n",
    "    \"WechatAgent\",\n",
    "    description=\"An agent for send message by wechat.\",\n",
    "    model_client=model_client,\n",
    "    tools=[wechat_send],\n",
    "    system_message=\"\"\"\n",
    "    You are a message send tool.\n",
    "    Given the tasks you have been assigned, you should extract the name and message when use this tool\n",
    "    \"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301e70d7-f93b-42bb-909b-59ce33ca0311",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_mention_termination = TextMentionTermination(\"TERMINATE\")\n",
    "max_messages_termination = MaxMessageTermination(max_messages=25)\n",
    "termination = text_mention_termination | max_messages_termination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7c2fa4-8804-4b84-bec2-d7f779a52d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "selector_prompt = \"\"\"Select an agent to perform task.\n",
    "\n",
    "{roles}\n",
    "\n",
    "Current conversation context:\n",
    "{history}\n",
    "\n",
    "Read the above conversation, then select an agent from {participants} to perform the next task.\n",
    "Make sure the planner agent has assigned tasks before other agents start working.\n",
    "Only select one agent.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85409ea6-f0ce-4ea5-9c77-79ec67313e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "team = SelectorGroupChat(\n",
    "    [planning_agent, web_search_agent, data_analyst_agent],\n",
    "    model_client=model_client,\n",
    "    termination_condition=termination,\n",
    "    selector_prompt=selector_prompt,\n",
    "    allow_repeated_speaker=True,  # Allow an agent to speak multiple turns in a row.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91617c6b-959e-4825-b478-fdfd8be3abfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"Who was the Miami Heat player with the highest points in the 2006-2007 season, and what was the percentage change in his total rebounds between the 2007-2008 and 2008-2009 seasons?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e58f50a-4177-4bbc-88b9-10debe638db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "await Console(team.run_stream(task=task))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e419a6d6-7d54-4843-a249-888f5e27a77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "def is_image_path(text):\n",
    "    image_extensions = (\".jpg\", \".jpeg\", \".png\", \".gif\", \".bmp\", \".tiff\", \".tif\")\n",
    "    if text.endswith(image_extensions):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def encode_image(image_path):\n",
    "    \"\"\"Encode image file to base64.\"\"\"\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
