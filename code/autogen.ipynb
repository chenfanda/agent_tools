{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40e87d25-4db5-43fa-aaa2-ff5dbac0a574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# from autogen import ConversableAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdbce53b-f137-4c52-a02e-93e3653145f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cathy = ConversableAgent(\n",
    "#     \"cathy\",\n",
    "#     system_message=\"Your name is Cathy and you are a part of a duo of comedians.\",\n",
    "#     llm_config={\"config_list\": [{\"model\": \"llama3.2:3b-instruct-q8_0\", \"base_url\":'http://10.100.165.224:11434/v1/',\"temperature\": 0.9, \"api_key\": 'ollama'}]},\n",
    "#     human_input_mode=\"NEVER\",  # Never ask for human input.\n",
    "# )\n",
    "\n",
    "# joe = ConversableAgent(\n",
    "#     \"joe\",\n",
    "#     system_message=\"Your name is Joe and you are a part of a duo of comedians.\",\n",
    "#     llm_config={\"config_list\": [{\"model\": \"llama3.2:3b-instruct-q8_0\", \"base_url\":'http://10.100.165.224:11434/v1/',\"temperature\": 0.7, \"api_key\": 'ollama'}]},\n",
    "#     human_input_mode=\"NEVER\",  # Never ask for human input.\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "747dc9c0-5ca5-4d16-9aa5-79da4e4d8b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = joe.initiate_chat(cathy, message=\"Cathy, tell me a joke.\", max_turns=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ce46c16-8064-4584-926a-811d8a8b71e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "flaml.automl is not available. Please install flaml[automl] to enable AutoML functionalities.\n"
     ]
    }
   ],
   "source": [
    "from autogen import AssistantAgent\n",
    "from autogen.agentchat.contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgent\n",
    "from chromadb.utils import embedding_functions\n",
    "from autogen.agentchat.contrib.vectordb.chromadb import ChromaVectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d103ea03-925d-49ca-99fc-b292f39251f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen.retrieve_utils import create_vector_db_from_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3296ea30-f178-4bd7-9035-86294001b17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6d1a7d74-d473-4a8e-a77a-7d358ba70983",
   "metadata": {},
   "outputs": [],
   "source": [
    "rct=RecursiveCharacterTextSplitter(separators=['\\n','\\t','\\r'],chunk_size=500,chunk_overlap=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "193b05c2-0a46-458e-a701-8c144b96d723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ollama_ef(['hi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d53ac7f8-5a1a-4b34-bf57-21295f31265a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ollama_ef(['2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "712b11cf-e68e-49a1-bfcd-ffb2bfeaa177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"https://huggingface.co/datasets/thinkall/NaturalQuestionsQA/resolve/main/corpus.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c47a8b7d-3cac-46cc-bcee-823dbc391fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d8137693-9eb3-43f7-8981-1378ad0555c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0591e8ec-f7d5-4bd6-85f6-73f3683957d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc=tiktoken.get_encoding('cl100k_base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "71f59962-37e2-4554-954d-7f4d7abec237",
   "metadata": {},
   "outputs": [],
   "source": [
    "client=create_vector_db_from_dir(dir_path=['/oper/ch/data/'],\n",
    "                          max_tokens=4000,\n",
    "                          chunk_mode=\"multi_lines\",\n",
    "                          db_path='/oper/ch/data/chromadb/',\n",
    "                          collection_name='autogen',\n",
    "                          get_or_create=True,\n",
    "                          embedding_function=ollama_ef,\n",
    "                          custom_text_split_function=rct.split_text,\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e618dcf-9462-4c1d-8292-aee46b5f5a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1='''--客户规模统计指标表\n",
    "CREATE TABLE IF NOT EXISTS \"P03_STAT_INDEX_DATA_CUST_D\" (\n",
    "\"Etl_dt\" datetime,\n",
    "\"lvl1_sub_cent_name\" text,\n",
    "\"lvl2_sub_cent_name\" text,\n",
    "\"CurrD_New_Cust_Cnt\" real,\n",
    "\"CurrM_New_Cust_Cnt\" real,\n",
    "\"CurrY_New_Cust_Cnt\" real,\n",
    "\"CurrY_Vald_Cust_Cnt\" real,\n",
    "\"CurrY_New_Cust_Vgrs30_Cnt\" real,\n",
    "\"Cust_Vgrs30_Cnt_NoNew\" real,\n",
    "\"Cust_Vgrs30_Cnt\" real,\n",
    "PRIMARY KEY (\"Etl_dt\",\"lvl1_sub_cent_name\",\"lvl2_sub_cent_name\")\n",
    ");'''\n",
    "a2='''--发卡规模统计指标表\n",
    "CREATE TABLE IF NOT EXISTS \"P03_STAT_INDEX_DATA_CARD_D\" (\n",
    "\"Etl_dt\" datetime,\n",
    "\"lvl1_sub_cent_name\" text,\n",
    "\"lvl2_sub_cent_name\" text,\n",
    "\"CurrD_New_Card_Cnt\" real,\n",
    "\"CurrM_New_Card_Cnt\" real,\n",
    "\"CurrY_New_Card_Cnt\" real,\n",
    "PRIMARY KEY (\"Etl_dt\",\"lvl1_sub_cent_name\",\"lvl2_sub_cent_name\")\n",
    ");'''\n",
    "a3='''--交易规模统计指标表\n",
    "CREATE TABLE IF NOT EXISTS \"P03_STAT_INDEX_DATA_TX_D\" (\n",
    "\"Etl_dt\" datetime,\n",
    "\"lvl1_sub_cent_name\" text,\n",
    "\"lvl2_sub_cent_name\" text,\n",
    "\"CurrD_Consm_Amt\" real,\n",
    "\"CurrM_Consm_Amt\" real,\n",
    "\"CurrY_Consm_Amt\" real,\n",
    "\"CurrD_Tcash_Amt\" real,\n",
    "\"CurrM_Tcash_Amt\" real,\n",
    "\"CurrY_Tcash_Amt\" real,\n",
    "\"CurrD_Sjj_Amt\" real,\n",
    "\"CurrM_Sjj_Amt\" real,\n",
    "\"CurrY_Sjj_Amt\" real,\n",
    "PRIMARY KEY (\"Etl_dt\",\"lvl1_sub_cent_name\",\"lvl2_sub_cent_name\")\n",
    ");'''\n",
    "a4='''--分期产品交易统计指标\n",
    "CREATE TABLE IF NOT EXISTS \"P03_STAT_INDEX_DATA_TX_STLM_D\" (\n",
    "\"Etl_dt\" datetime,\n",
    "\"lvl1_sub_cent_name\" text,\n",
    "\"lvl2_sub_cent_name\" text,\n",
    "\"Prod\" text,\n",
    "\"CurrD_Stlm_Amt\" real,\n",
    "\"CurrM_Stlm_Amt\" real,\n",
    "\"CurrY_Stlm_Amt\" real,\n",
    "PRIMARY KEY (\"Etl_dt\",\"lvl1_sub_cent_name\",\"lvl2_sub_cent_name\",\"Prod\")\n",
    ");'''\n",
    "a5='''--资产质量统计指标表\n",
    "CREATE TABLE IF NOT EXISTS \"P03_STAT_INDEX_DATA_AS_D\" (\n",
    "\"Etl_dt\" datetime,\n",
    "\"lvl1_sub_cent_name\" text,\n",
    "\"lvl2_sub_cent_name\" text,\n",
    "\"Curr_Loan_Bal\" real,\n",
    "\"Curr_Ovrd_Bal\" real,\n",
    "\"Curr_Np_Amt\" real,\n",
    "\"Curr_Np_Amt_In\" real,\n",
    "\"Curr_Np_Amt_In_Out\" real,\n",
    "\"CurrY_Np_ABS_Tran_Amt\" real,\n",
    "\"CurrY_Norm_Wrto_Amt\" real,\n",
    "\"CurrD_Np_Rpy_Amt\" real,\n",
    "\"CurrM_Np_Rpy_Amt\" real,\n",
    "\"CurrY_Np_Rpy_Amt\" real,\n",
    "\"CurrD_Wrto_Rpy_Amt\" real,\n",
    "\"CurrM_Wrto_Rpy_Amt\" real,\n",
    "\"CurrY_Wrto_Rpy_Amt\" real,\n",
    "PRIMARY KEY (\"Etl_dt\",\"lvl1_sub_cent_name\",\"lvl2_sub_cent_name\")\n",
    ");'''\n",
    "a6='''--同业银行统计指标表\n",
    "CREATE TABLE IF NOT EXISTS \"P03_STAT_INDEX_DATA_IBANK_D\" (\n",
    "\"Etl_dt\" datetime,\n",
    "\"Ibank_Bank_Name\" text,\n",
    "\"Kpi_Name\" text,\n",
    "\"Kpi_Val\" real,\n",
    "\"Kpi_LstQEVal\" real,\n",
    "\"Kpi_LstQE_Incr\" real,\n",
    "\"Kpi_LstQE_Rate\" real,\n",
    "\"Kpi_LstYEVal\" real,len(\n",
    "\"Kpi_LstYE_Incr\" real,\n",
    "\"Kpi_LstYE_Rate\" real,\n",
    "PRIMARY KEY (\"Etl_dt\",\"Ibank_Bank_Name\",\"Kpi_Name\")\n",
    ");'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7005a03-586e-4ebb-b7d3-f21c6f971090",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = chromadb.PersistentClient(path=\"/oper/ch/data/chromadb/\")\n",
    "col = client.get_or_create_collection(\"tabel_collection\",embedding_function=ollama_ef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "231fd1d0-bb03-431e-b488-a98ad525f0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Insert of existing embedding ID: 客户规模统计指标表\n",
      "Insert of existing embedding ID: 发卡规模统计指标表\n",
      "Insert of existing embedding ID: 交易规模统计指标表\n",
      "Insert of existing embedding ID: 分期产品交易统计指标\n",
      "Insert of existing embedding ID: 资产质量统计指标表\n",
      "Insert of existing embedding ID: 同业银行统计指标表\n",
      "Add of existing embedding ID: 客户规模统计指标表\n",
      "Add of existing embedding ID: 发卡规模统计指标表\n",
      "Add of existing embedding ID: 交易规模统计指标表\n",
      "Add of existing embedding ID: 分期产品交易统计指标\n",
      "Add of existing embedding ID: 资产质量统计指标表\n",
      "Add of existing embedding ID: 同业银行统计指标表\n"
     ]
    }
   ],
   "source": [
    "col.add(ids=ids, documents=[a1,a2,a3,a4,a5,a6], embeddings=ollama_ef([a1,a2,a3,a4,a5,a6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bcc5a348-e152-4f21-b92f-d779998c6eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=ollama_ef([a1,a2,a3,a4,a5,a6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f505b5f-0ca8-4831-bee8-4189ab8e8725",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 10 is greater than number of elements in index 6, updating n_results = 6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ids': [['客户规模统计指标表',\n",
       "   '同业银行统计指标表',\n",
       "   '交易规模统计指标表',\n",
       "   '资产质量统计指标表',\n",
       "   '分期产品交易统计指标',\n",
       "   '发卡规模统计指标表']],\n",
       " 'embeddings': None,\n",
       " 'documents': [['--客户规模统计指标表\\nCREATE TABLE IF NOT EXISTS \"P03_STAT_INDEX_DATA_CUST_D\" (\\n\"Etl_dt\" datetime,\\n\"lvl1_sub_cent_name\" text,\\n\"lvl2_sub_cent_name\" text,\\n\"CurrD_New_Cust_Cnt\" real,\\n\"CurrM_New_Cust_Cnt\" real,\\n\"CurrY_New_Cust_Cnt\" real,\\n\"CurrY_Vald_Cust_Cnt\" real,\\n\"CurrY_New_Cust_Vgrs30_Cnt\" real,\\n\"Cust_Vgrs30_Cnt_NoNew\" real,\\n\"Cust_Vgrs30_Cnt\" real,\\nPRIMARY KEY (\"Etl_dt\",\"lvl1_sub_cent_name\",\"lvl2_sub_cent_name\")\\n);',\n",
       "   '--同业银行统计指标表\\nCREATE TABLE IF NOT EXISTS \"P03_STAT_INDEX_DATA_IBANK_D\" (\\n\"Etl_dt\" datetime,\\n\"Ibank_Bank_Name\" text,\\n\"Kpi_Name\" text,\\n\"Kpi_Val\" real,\\n\"Kpi_LstQEVal\" real,\\n\"Kpi_LstQE_Incr\" real,\\n\"Kpi_LstQE_Rate\" real,\\n\"Kpi_LstYEVal\" real,\\n\"Kpi_LstYE_Incr\" real,\\n\"Kpi_LstYE_Rate\" real,\\nPRIMARY KEY (\"Etl_dt\",\"Ibank_Bank_Name\",\"Kpi_Name\")\\n);',\n",
       "   '--交易规模统计指标表\\nCREATE TABLE IF NOT EXISTS \"P03_STAT_INDEX_DATA_TX_D\" (\\n\"Etl_dt\" datetime,\\n\"lvl1_sub_cent_name\" text,\\n\"lvl2_sub_cent_name\" text,\\n\"CurrD_Consm_Amt\" real,\\n\"CurrM_Consm_Amt\" real,\\n\"CurrY_Consm_Amt\" real,\\n\"CurrD_Tcash_Amt\" real,\\n\"CurrM_Tcash_Amt\" real,\\n\"CurrY_Tcash_Amt\" real,\\n\"CurrD_Sjj_Amt\" real,\\n\"CurrM_Sjj_Amt\" real,\\n\"CurrY_Sjj_Amt\" real,\\nPRIMARY KEY (\"Etl_dt\",\"lvl1_sub_cent_name\",\"lvl2_sub_cent_name\")\\n);',\n",
       "   '--资产质量统计指标表\\nCREATE TABLE IF NOT EXISTS \"P03_STAT_INDEX_DATA_AS_D\" (\\n\"Etl_dt\" datetime,\\n\"lvl1_sub_cent_name\" text,\\n\"lvl2_sub_cent_name\" text,\\n\"Curr_Loan_Bal\" real,\\n\"Curr_Ovrd_Bal\" real,\\n\"Curr_Np_Amt\" real,\\n\"Curr_Np_Amt_In\" real,\\n\"Curr_Np_Amt_In_Out\" real,\\n\"CurrY_Np_ABS_Tran_Amt\" real,\\n\"CurrY_Norm_Wrto_Amt\" real,\\n\"CurrD_Np_Rpy_Amt\" real,\\n\"CurrM_Np_Rpy_Amt\" real,\\n\"CurrY_Np_Rpy_Amt\" real,\\n\"CurrD_Wrto_Rpy_Amt\" real,\\n\"CurrM_Wrto_Rpy_Amt\" real,\\n\"CurrY_Wrto_Rpy_Amt\" real,\\nPRIMARY KEY (\"Etl_dt\",\"lvl1_sub_cent_name\",\"lvl2_sub_cent_name\")\\n);',\n",
       "   '--分期产品交易统计指标\\nCREATE TABLE IF NOT EXISTS \"P03_STAT_INDEX_DATA_TX_STLM_D\" (\\n\"Etl_dt\" datetime,\\n\"lvl1_sub_cent_name\" text,\\n\"lvl2_sub_cent_name\" text,\\n\"Prod\" text,\\n\"CurrD_Stlm_Amt\" real,\\n\"CurrM_Stlm_Amt\" real,\\n\"CurrY_Stlm_Amt\" real,\\nPRIMARY KEY (\"Etl_dt\",\"lvl1_sub_cent_name\",\"lvl2_sub_cent_name\",\"Prod\")\\n);',\n",
       "   '--发卡规模统计指标表\\nCREATE TABLE IF NOT EXISTS \"P03_STAT_INDEX_DATA_CARD_D\" (\\n\"Etl_dt\" datetime,\\n\"lvl1_sub_cent_name\" text,\\n\"lvl2_sub_cent_name\" text,\\n\"CurrD_New_Card_Cnt\" real,\\n\"CurrM_New_Card_Cnt\" real,\\n\"CurrY_New_Card_Cnt\" real,\\nPRIMARY KEY (\"Etl_dt\",\"lvl1_sub_cent_name\",\"lvl2_sub_cent_name\")\\n);']],\n",
       " 'uris': None,\n",
       " 'data': None,\n",
       " 'metadatas': [[None, None, None, None, None, None]],\n",
       " 'distances': [[366.0380494537008,\n",
       "   589.1172291588157,\n",
       "   611.884509427782,\n",
       "   614.2372884706482,\n",
       "   710.5546245408684,\n",
       "   748.3650628603834]],\n",
       " 'included': [<IncludeEnum.distances: 'distances'>,\n",
       "  <IncludeEnum.documents: 'documents'>,\n",
       "  <IncludeEnum.metadatas: 'metadatas'>]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col.query(ollama_ef(['客户报表']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfe84917-473f-4c20-ac2d-5880b75b1aae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': ['客户规模统计指标表',\n",
       "  '发卡规模统计指标表',\n",
       "  '交易规模统计指标表',\n",
       "  '分期产品交易统计指标',\n",
       "  '资产质量统计指标表',\n",
       "  '同业银行统计指标表'],\n",
       " 'embeddings': None,\n",
       " 'documents': ['--客户规模统计指标表\\nCREATE TABLE IF NOT EXISTS \"P03_STAT_INDEX_DATA_CUST_D\" (\\n\"Etl_dt\" datetime,\\n\"lvl1_sub_cent_name\" text,\\n\"lvl2_sub_cent_name\" text,\\n\"CurrD_New_Cust_Cnt\" real,\\n\"CurrM_New_Cust_Cnt\" real,\\n\"CurrY_New_Cust_Cnt\" real,\\n\"CurrY_Vald_Cust_Cnt\" real,\\n\"CurrY_New_Cust_Vgrs30_Cnt\" real,\\n\"Cust_Vgrs30_Cnt_NoNew\" real,\\n\"Cust_Vgrs30_Cnt\" real,\\nPRIMARY KEY (\"Etl_dt\",\"lvl1_sub_cent_name\",\"lvl2_sub_cent_name\")\\n);',\n",
       "  '--发卡规模统计指标表\\nCREATE TABLE IF NOT EXISTS \"P03_STAT_INDEX_DATA_CARD_D\" (\\n\"Etl_dt\" datetime,\\n\"lvl1_sub_cent_name\" text,\\n\"lvl2_sub_cent_name\" text,\\n\"CurrD_New_Card_Cnt\" real,\\n\"CurrM_New_Card_Cnt\" real,\\n\"CurrY_New_Card_Cnt\" real,\\nPRIMARY KEY (\"Etl_dt\",\"lvl1_sub_cent_name\",\"lvl2_sub_cent_name\")\\n);',\n",
       "  '--交易规模统计指标表\\nCREATE TABLE IF NOT EXISTS \"P03_STAT_INDEX_DATA_TX_D\" (\\n\"Etl_dt\" datetime,\\n\"lvl1_sub_cent_name\" text,\\n\"lvl2_sub_cent_name\" text,\\n\"CurrD_Consm_Amt\" real,\\n\"CurrM_Consm_Amt\" real,\\n\"CurrY_Consm_Amt\" real,\\n\"CurrD_Tcash_Amt\" real,\\n\"CurrM_Tcash_Amt\" real,\\n\"CurrY_Tcash_Amt\" real,\\n\"CurrD_Sjj_Amt\" real,\\n\"CurrM_Sjj_Amt\" real,\\n\"CurrY_Sjj_Amt\" real,\\nPRIMARY KEY (\"Etl_dt\",\"lvl1_sub_cent_name\",\"lvl2_sub_cent_name\")\\n);',\n",
       "  '--分期产品交易统计指标\\nCREATE TABLE IF NOT EXISTS \"P03_STAT_INDEX_DATA_TX_STLM_D\" (\\n\"Etl_dt\" datetime,\\n\"lvl1_sub_cent_name\" text,\\n\"lvl2_sub_cent_name\" text,\\n\"Prod\" text,\\n\"CurrD_Stlm_Amt\" real,\\n\"CurrM_Stlm_Amt\" real,\\n\"CurrY_Stlm_Amt\" real,\\nPRIMARY KEY (\"Etl_dt\",\"lvl1_sub_cent_name\",\"lvl2_sub_cent_name\",\"Prod\")\\n);',\n",
       "  '--资产质量统计指标表\\nCREATE TABLE IF NOT EXISTS \"P03_STAT_INDEX_DATA_AS_D\" (\\n\"Etl_dt\" datetime,\\n\"lvl1_sub_cent_name\" text,\\n\"lvl2_sub_cent_name\" text,\\n\"Curr_Loan_Bal\" real,\\n\"Curr_Ovrd_Bal\" real,\\n\"Curr_Np_Amt\" real,\\n\"Curr_Np_Amt_In\" real,\\n\"Curr_Np_Amt_In_Out\" real,\\n\"CurrY_Np_ABS_Tran_Amt\" real,\\n\"CurrY_Norm_Wrto_Amt\" real,\\n\"CurrD_Np_Rpy_Amt\" real,\\n\"CurrM_Np_Rpy_Amt\" real,\\n\"CurrY_Np_Rpy_Amt\" real,\\n\"CurrD_Wrto_Rpy_Amt\" real,\\n\"CurrM_Wrto_Rpy_Amt\" real,\\n\"CurrY_Wrto_Rpy_Amt\" real,\\nPRIMARY KEY (\"Etl_dt\",\"lvl1_sub_cent_name\",\"lvl2_sub_cent_name\")\\n);',\n",
       "  '--同业银行统计指标表\\nCREATE TABLE IF NOT EXISTS \"P03_STAT_INDEX_DATA_IBANK_D\" (\\n\"Etl_dt\" datetime,\\n\"Ibank_Bank_Name\" text,\\n\"Kpi_Name\" text,\\n\"Kpi_Val\" real,\\n\"Kpi_LstQEVal\" real,\\n\"Kpi_LstQE_Incr\" real,\\n\"Kpi_LstQE_Rate\" real,\\n\"Kpi_LstYEVal\" real,\\n\"Kpi_LstYE_Incr\" real,\\n\"Kpi_LstYE_Rate\" real,\\nPRIMARY KEY (\"Etl_dt\",\"Ibank_Bank_Name\",\"Kpi_Name\")\\n);'],\n",
       " 'uris': None,\n",
       " 'data': None,\n",
       " 'metadatas': [None, None, None, None, None, None],\n",
       " 'included': [<IncludeEnum.documents: 'documents'>,\n",
       "  <IncludeEnum.metadatas: 'metadatas'>]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e271c76-59e1-4bda-8bac-61396362f5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids=['客户规模统计指标表','发卡规模统计指标表','交易规模统计指标表','分期产品交易统计指标','资产质量统计指标表','同业银行统计指标表']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de083f63-7eda-4ed2-934a-3957c942b74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_ef=embedding_functions.OllamaEmbeddingFunction(url='http://localhost:11434/api/embeddings',model_name='m3e:latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0a4d2e8-bdd1-48b6-a9cd-f2266659d500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ollama_ef(['hi','你好']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b30b12d4-7bb7-4746-b743-623fb8dca769",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_config = {\n",
    "    \"config_list\": [\n",
    "        {\n",
    "            \"model\": \"minicpm3:4b\",\n",
    "            \"base_url\":'http://localhost:11434/v1/',\n",
    "            \"api_key\": 'ollama',\n",
    "        },\n",
    "    ]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1701e959-f573-4cc6-8813-0d9d35c436e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant=AssistantAgent('assistant',system_message='你是优秀的助手',llm_config=llm_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ea320795-f7d5-4455-b309-da67c0b1ffb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from autogen.agentchat.contrib.vectordb.chromadb import ChromaVectorDB\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "dcdac251-9ebd-4ff3-a093-e656c96709ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "recur_spliter = RecursiveCharacterTextSplitter(separators=[\"\\n\", \"\\r\", \"\\t\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1f37203-e536-4124-a5e0-34e65e278770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install fastapi -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02d1f318-92f1-4df8-a2fa-7111bdfe47b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6ef67983-c69d-4dc1-adfe-a92ffa62099f",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHROMA_DB_PATH=\"/oper/ch/data/chromadb\"\n",
    "CHROMA_COLLECTION=\"autogen-docs-test\"\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path=CHROMA_DB_PATH)\n",
    "collection = chroma_client.get_or_create_collection(name=CHROMA_COLLECTION)\n",
    "\n",
    "ollama_ef = embedding_functions.OllamaEmbeddingFunction(\n",
    "        url=\"http://localhost:11434/api/embeddings\",\n",
    "        model_name=\"m3e:latest\",\n",
    "        )\n",
    "vector_db = ChromaVectorDB(path=CHROMA_DB_PATH, embedding_function = ollama_ef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2bb51170-85b1-48d6-9ca5-cd75b5de3749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ragproxyagent = RetrieveUserProxyAgent(\n",
    "#     name=\"ragproxyagent\",\n",
    "#     human_input_mode=\"NEVER\",\n",
    "#     # llm_config=llm_config,\n",
    "#     code_execution_config=False,\n",
    "#     retrieve_config={\n",
    "#         # \"model\": config_list[0][\"model\"],\n",
    "#         \"task\": \"qa\",\n",
    "#         \"update_context\": True,\n",
    "#         \"n_results\": 3,\n",
    "#         \"new_docs\":[\"/oper/public/data/create_table2.sql\"],\n",
    "#         \"docs_path\":[\"/oper/public/data/create_table2.sql\",],\n",
    "#         \"custom_text_split_function\": recur_spliter.split_text,\n",
    "#         \"custom_text_type\": ['txt'],\n",
    "#        \"get_or_create\": True,\n",
    "#        \"overwrite\": False,\n",
    "#        \"vector_db\": vector_db,\n",
    "#        \"collection_name\": CHROMA_COLLECTION,\n",
    "#        \"embedding_function\": ollama_ef,\n",
    "#     },\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc725be3-61e9-4459-994e-730913a4a6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_db.embedding_function(['hi'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abdb9b46-6260-45eb-9215-2c0e4ad076e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ollama_ef(['hi'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8c9ba1c-9967-4bbf-8fa5-4dba093f7c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install autogen-agentchat -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e062f2b-b10b-428c-9b1e-4e23aaab86fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install chromadb==0.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f3c1e0e0-23e8-4578-9e27-de56e584d658",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "be3f0f9b-4063-4663-bfb7-8b6c430908d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "91dc9ae6-e3f0-4730-8fd7-930ff333f005",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OllamaEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5eab18ad-92bd-4cb4-a2b6-38c96e81d0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "849adc94-8de7-4b0d-bc8d-cfeee61316f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "81b8148b-33b6-41de-86db-225c19389c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter =RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "49200622-23d6-47a5-8fa4-cae8c89e8b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/oper/public/data/create_table2.sql','r') as file_handle:\n",
    "    table_info=file_handle.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3e29d6ee-b71d-4089-9aff-02b94f528a60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2390"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'\\n'.join(table_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "497c90d6-6a92-4623-aa58-0d52f443eb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents=[Document(page_content='\\n'.join(table_info),id=0,metadata={\"source\":'tabel_info'})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "48101603-89c0-4e48-8d51-bfb99645a563",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_doc_splits= text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "168ca7c1-c5b0-4d76-8760-239a89754abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2930461/3012661940.py:1: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  embedding = OllamaEmbeddings(base_url='http://localhost:11434',model='m3e:latest')\n"
     ]
    }
   ],
   "source": [
    "embedding = OllamaEmbeddings(base_url='http://localhost:11434',model='m3e:latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fc0abb5b-a504-41e4-b226-8d1b39f89cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore=Chroma.from_documents(documents=all_doc_splits,persist_directory='/oper/ch/data/chromadb',embedding=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "af285221-6d15-47f6-8e03-2ca6b7bfaa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorstore.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "38ec8ffa-fd5a-4814-9b73-1137c7925b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ragproxyagent = RetrieveUserProxyAgent(\n",
    "    name=\"ragproxyagent\",\n",
    "    # human_input_mode=\"NEVER\",\n",
    "    # max_consecutive_auto_reply=3,\n",
    "    retrieve_config={\n",
    "        \"task\": \"qa\",\n",
    "        \"update_context\":True,\n",
    "        \"docs_path\": ['/oper/public/create_table2.sql'],\n",
    "        # \"vector_db\": ChromaVectorDB(path='/oper/ch/data/chromadb',embedding_function=ollama_ef),\n",
    "        \"client\": chromadb.PersistentClient(path=\"/oper/ch/data/chromadb/\"),\n",
    "        \"embedding_function\": ollama_ef,\n",
    "        \"model\": llm_config['config_list'][0][\"model\"],\n",
    "        \"n_results\": 1,\n",
    "        \"get_or_create\": True,\n",
    "        # \"custom_text_split_function\": recur_spliter.split_text,\n",
    "        \"collection_name\": \"tabel_collection\",\n",
    "        \"custom_text_type\": ['txt']\n",
    "    },\n",
    "    code_execution_config=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c267d198-b9c1-4f67-9498-08aeb84948f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VectorDB returns doc_ids:  [[]]\n",
      "\u001b[32mNo more context, will terminate.\u001b[0m\n",
      "\u001b[33mragproxyagent\u001b[0m (to assistant):\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatResult(chat_id=None, chat_history=[{'content': 'TERMINATE', 'role': 'assistant', 'name': 'ragproxyagent'}], summary='', cost={'usage_including_cached_inference': {'total_cost': 0}, 'usage_excluding_cached_inference': {'total_cost': 0}}, human_input=[])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assistant.reset()\n",
    "ragproxyagent.initiate_chat(assistant,message=ragproxyagent.message_generator,problem='表')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb3eb780-2615-4404-93a9-7b48204adcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat /oper/ch/env/autogen/lib/python3.10/site-packages/chromadb/api/models/Collection.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "953a3423-0bf0-451d-b709-de1e0dd49736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install chromadb==0.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2c87f313-8bc7-49ca-9d50-cadbd8788d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# while True:\n",
    "#     problem = input(\"Please enter your question: \")\n",
    "#     if problem == \"\":\n",
    "#         break\n",
    "#     assistant.reset()\n",
    "#     ragproxyagent.initiate_chat(assistant, message=ragproxyagent.message_generator, problem=problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "540e986d-d4cf-44ae-a79e-e083c6876e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_list=[{'model': 'minicpm3:4b',\n",
    "   'base_url': 'http://localhost:11434/v1/',\n",
    "   'api_key': 'ollama'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94a06a63-4ad0-4b7b-bd04-a4bb72226cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. create an AssistantAgent instance named \"assistant\"\n",
    "assistant = AssistantAgent(\n",
    "    name=\"assistant\",\n",
    "    system_message=\"You are a helpful assistant.\",\n",
    "    llm_config={\n",
    "        \"timeout\": 600,\n",
    "        \"cache_seed\": 42,\n",
    "        \"config_list\": config_list,\n",
    "    },\n",
    ")\n",
    "\n",
    "# 2. create the RetrieveUserProxyAgent instance named \"ragproxyagent\"\n",
    "# Refer to https://microsoft.github.io/autogen/docs/reference/agentchat/contrib/retrieve_user_proxy_agent\n",
    "# and https://microsoft.github.io/autogen/docs/reference/agentchat/contrib/vectordb/chromadb\n",
    "# for more information on the RetrieveUserProxyAgent and ChromaVectorDB\n",
    "ragproxyagent = RetrieveUserProxyAgent(\n",
    "    name=\"ragproxyagent\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=3,\n",
    "    retrieve_config={\n",
    "        \"task\": \"code\",\n",
    "        \"docs_path\": [\n",
    "            \"https://raw.githubusercontent.com/microsoft/FLAML/main/website/docs/Examples/Integrate%20-%20Spark.md\",\n",
    "            \"https://raw.githubusercontent.com/microsoft/FLAML/main/website/docs/Research.md\",\n",
    "        ],\n",
    "        \"chunk_token_size\": 2000,\n",
    "        \"model\": config_list[0][\"model\"],\n",
    "        \"vector_db\": \"chroma\",\n",
    "        \"overwrite\": False,  # set to True if you want to overwrite an existing collection\n",
    "        \"get_or_create\": True,  # set to False if don't want to reuse an existing collection\n",
    "    },\n",
    "    code_execution_config=False,  # set to False if you don't want to execute the code\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d47d15-796c-49ef-8681-5be460c4f3f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70ff99dd-2735-478c-8b91-52369aadca7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ragproxyagent = RetrieveUserProxyAgent(\n",
    "    name=\"ragproxyagent\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=3,\n",
    "    retrieve_config={\n",
    "        \"task\": \"code\",\n",
    "        \"docs_path\": [\n",
    "            \"https://raw.githubusercontent.com/microsoft/FLAML/main/website/docs/Examples/Integrate%20-%20Spark.md\",\n",
    "            \"https://raw.githubusercontent.com/microsoft/FLAML/main/website/docs/Research.md\",\n",
    "        ],\n",
    "        \"chunk_token_size\": 2000,\n",
    "        # \"model\": config_list[0][\"model\"],\n",
    "        \"vector_db\":ChromaVectorDB(path='/oper/ch/data/chromadb',embedding_function=ollama_ef),\n",
    "        # \"vector_db\": \"chroma\",\n",
    "        \"collection_name\": \"tabel_collection\",\n",
    "        \"embedding_function\": ollama_ef,\n",
    "        \"overwrite\": False,  # set to True if you want to overwrite an existing collection\n",
    "        \"get_or_create\": True,  # set to False if don't want to reuse an existing collection\n",
    "    },\n",
    "    code_execution_config=False,  # set to False if you don't want to execute the code\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab356aa5-4983-4bc1-b631-5c33567f7ffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ollama_ef(['hi'])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd32b4a-15d0-4117-b82b-fb6ae446f019",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_ef=embedding_functions.OllamaEmbeddingFunction(url='http://localhost:11434/api/embeddings',model_name='m3e:latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f348cba8-ab85-4f34-ae2d-a624f974d40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64a50bd3-f002-4d9d-a4cb-8334609ca176",
   "metadata": {},
   "outputs": [],
   "source": [
    "work_dir=Path('/oper/ch/autogen/coding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dc74f4a-0673-4172-85cb-dafb88576808",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "flaml.automl is not available. Please install flaml[automl] to enable AutoML functionalities.\n"
     ]
    }
   ],
   "source": [
    "from autogen.coding import CodeBlock, LocalCommandLineCodeExecutor,DockerCommandLineCodeExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cb77ae4-618e-4555-aec8-0a29b978fc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen import ConversableAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2400b38f-6ceb-48a5-b05c-cd4e9f83b553",
   "metadata": {},
   "outputs": [],
   "source": [
    "executor = DockerCommandLineCodeExecutor(image='python:3.12-slim',\n",
    "                              work_dir=work_dir)\n",
    "# executor = DockerCommandLineCodeExecutor(image='cuda121_python3.10',\n",
    "#                               work_dir=work_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a811460-ec12-4f52-8ce9-2b188075a2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "work_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "77ec94c9-cabd-4816-a065-cb795084e166",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_executor_agent = ConversableAgent(\n",
    "    name=\"code_executor_agent\",\n",
    "    llm_config=False,\n",
    "    code_execution_config={\n",
    "        \"executor\": executor,\n",
    "    },\n",
    "    human_input_mode=\"NEVER\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a9522577-5b33-4602-9a8f-7ddf807e73bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_list=[{'model': 'qwq:32b-preview-q8_0',\n",
    "  'base_url': 'http://localhost:11434/v1/',\n",
    "  'api_key': 'ollama'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "fbc69fbb-570c-440c-820f-3fb4266f55e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code writer agent's system message is to instruct the LLM on how to\n",
    "# use the Jupyter code executor with IPython kernel.\n",
    "code_writer_system_message = \"\"\"\n",
    "You have been given coding capability to solve tasks using Python code.\n",
    "In the following cases, suggest python code (in a python coding block) or shell script (in a sh coding block) for the user to execute.\n",
    "    1. When you need to collect info, use the code to output the info you need, for example, browse or search the web, download/read a file, print the content of a webpage or a file, get the current date/time, check the operating system. After sufficient info is printed and the task is ready to be solved based on your language skill, you can solve the task by yourself.\n",
    "    2. When you need to perform some task with code, use the code to perform the task and output the result. Finish the task smartly.\n",
    "Solve the task step by step if you need to. If a plan is not provided, explain your plan first. Be clear which step uses code, and which step uses your language skill.\n",
    "When using code, you must indicate the script type in the code block. The user cannot provide any other feedback or perform any other action beyond executing the code you suggest. The user can't modify your code. So do not suggest incomplete code which requires users to modify. Don't use a code block if it's not intended to be executed by the user.\n",
    "If you want the user to save the code in a file before executing it, put # filename: <filename> inside the code block as the first line. Don't include multiple code blocks in one response. Do not ask users to copy and paste the result. Instead, use 'print' function for the output when relevant. Check the execution result returned by the user.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "code_writer_agent = ConversableAgent(\n",
    "    \"code_writer\",\n",
    "    system_message=code_writer_system_message,\n",
    "    llm_config={\"config_list\": config_list,\"timeout\": 10000,},\n",
    "    code_execution_config=False,  # Turn off code execution for this agent.\n",
    "    max_consecutive_auto_reply=20,\n",
    "    human_input_mode=\"NEVER\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c1afcc28-1617-4bb7-ade1-0801d53b3227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "today = datetime.datetime.now().strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3dfc8b78-2178-46d6-a94f-498a121562a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pprint\n",
    "\n",
    "# chat_result = code_executor_agent.initiate_chat(\n",
    "#     code_writer_agent, message=\"Write Python code to calculate the 14th Fibonacci number.\"\n",
    "# )\n",
    "\n",
    "# pprint.pprint(chat_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b6bd8188-f83a-4fb4-8027-85b28471dd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_result = code_executor_agent.initiate_chat(\n",
    "    code_writer_agent,\n",
    "    message=f\"Today is {today}. Write Python code to plot TSLA's and META's \"\n",
    "    \"stock price gains YTD, and save the plot to a file named 'stock_gains.png'.\",\n",
    ")\n",
    "pprint.pprint(chat_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ece8a809-6259-43d0-9606-2076ea0de2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "executor.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40970113-bb4a-4872-9d00-1ca6c4505efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# executor = LocalCommandLineCodeExecutor(work_dir=work_dir)\n",
    "# print(\n",
    "#     executor.execute_code_blocks(\n",
    "#         code_blocks=[\n",
    "#             CodeBlock(language=\"python\", code=\"print('Hello, World!')\"),\n",
    "#         ]\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4ab1dcd0-342c-4b51-aedf-acdc0e74b384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exit_code=0 output='/oper/ch/autogen/agent_env/bin/python\\n' code_file='/oper/ch/code/tmp_code_278b725fd664fea2ecbf552b6b17cf04.py'\n"
     ]
    }
   ],
   "source": [
    "from autogen.code_utils import create_virtual_env\n",
    "from autogen.coding import CodeBlock, LocalCommandLineCodeExecutor\n",
    "\n",
    "venv_dir = \"/oper/ch/autogen/agent_env\"\n",
    "venv_context = create_virtual_env(venv_dir)\n",
    "executor = LocalCommandLineCodeExecutor(virtual_env_context=venv_context)\n",
    "print(\n",
    "    executor.execute_code_blocks(code_blocks=[CodeBlock(language=\"python\", code=\"import sys; print(sys.executable)\")])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c53a697-db01-4a30-aa6a-248c7e817bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assistant.reset()\n",
    "\n",
    "# qa_problem = \"Who is the author of FLAML?\"\n",
    "# chat_result = ragproxyagent.initiate_chat(assistant, message=ragproxyagent.message_generator, problem=qa_problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9b67f3-2a08-45a1-84d6-00536b9b11e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "boss = autogen.UserProxyAgent(\n",
    "    name=\"Boss\",\n",
    "    is_termination_msg=termination_msg,\n",
    "    human_input_mode=\"TERMINATE\",\n",
    "    system_message=\"The boss who ask questions and give tasks.\",\n",
    ")\n",
    "\n",
    "boss_aid = RetrieveUserProxyAgent(\n",
    "    name=\"Boss_Assistant\",\n",
    "    is_termination_msg=termination_msg,\n",
    "    system_message=\"Assistant who has extra content retrieval power for solving difficult problems.\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=3,\n",
    "    retrieve_config={\n",
    "        \"task\": \"qa\",\n",
    "    },\n",
    "    code_execution_config=False,  # we don't want to execute code in this case.\n",
    ")\n",
    "\n",
    "coder = autogen.AssistantAgent(\n",
    "    name=\"Senior_Python_Engineer\",\n",
    "    is_termination_msg=termination_msg,\n",
    "    system_message=\"You are a senior python engineer. Reply `TERMINATE` in the end when everything is done.\",\n",
    "    llm_config={\"config_list\": config_list, \"timeout\": 60, \"temperature\": 0},\n",
    ")\n",
    "\n",
    "pm = autogen.AssistantAgent(\n",
    "    name=\"Product_Manager\",\n",
    "    is_termination_msg=termination_msg,\n",
    "    system_message=\"You are a product manager. Reply `TERMINATE` in the end when everything is done.\",\n",
    "    llm_config={\"config_list\": config_list, \"timeout\": 60, \"temperature\": 0},\n",
    ")\n",
    "\n",
    "reviewer = autogen.AssistantAgent(\n",
    "    name=\"Code_Reviewer\",\n",
    "    is_termination_msg=termination_msg,\n",
    "    system_message=\"You are a code reviewer. Reply `TERMINATE` in the end when everything is done.\",\n",
    "    llm_config={\"config_list\": config_list, \"timeout\": 60, \"temperature\": 0},\n",
    ")\n",
    "\n",
    "def retrieve_content(\n",
    "    message: Annotated[\n",
    "        str,\n",
    "        \"Refined message which keeps the original meaning and can be used to retrieve content for code generation and question answering.\",\n",
    "    ],\n",
    "    n_results: Annotated[int, \"number of results\"] = 3,\n",
    ") -> str:\n",
    "    boss_aid.n_results = n_results  # Set the number of results to be retrieved.\n",
    "    _context = {\"problem\": message, \"n_results\": n_results}\n",
    "    ret_msg = boss_aid.message_generator(boss_aid, None, _context)\n",
    "    return ret_msg or message\n",
    "\n",
    "for caller in [pm, coder, reviewer]:\n",
    "    d_retrieve_content = caller.register_for_llm(\n",
    "        description=\"retrieve content for code generation and question answering.\", api_style=\"function\"\n",
    "    )(retrieve_content)\n",
    "\n",
    "for executor in [boss, pm]:\n",
    "    executor.register_for_execution()(d_retrieve_content)\n",
    "\n",
    "groupchat = autogen.GroupChat(\n",
    "    agents=[boss, pm, coder, reviewer],\n",
    "    messages=[],\n",
    "    max_round=12,\n",
    "    speaker_selection_method=\"round_robin\",\n",
    "    allow_repeat_speaker=False,\n",
    ")\n",
    "\n",
    "llm_config = {\"config_list\": config_list, \"timeout\": 60, \"temperature\": 0}\n",
    "manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config)\n",
    "\n",
    "# Start chatting with the boss as this is the user proxy agent.\n",
    "boss.initiate_chat(\n",
    "    manager,\n",
    "    message=\"How to use spark for parallel training in FLAML? Give me sample code.\",\n",
    ")\n",
    "# Initialize Agents\n",
    "def initialize_agents(config_list, docs_path=None):\n",
    "    ...\n",
    "    return assistant, ragproxyagent\n",
    "\n",
    "# Initialize Chat\n",
    "def initiate_chat(config_list, problem, queue, n_results=3):\n",
    "    ...\n",
    "    assistant.reset()\n",
    "    try:\n",
    "        ragproxyagent.a_initiate_chat(\n",
    "            assistant, problem=problem, silent=False, n_results=n_results\n",
    "        )\n",
    "        messages = ragproxyagent.chat_messages\n",
    "        messages = [messages[k] for k in messages.keys()][0]\n",
    "        messages = [m[\"content\"] for m in messages if m[\"role\"] == \"user\"]\n",
    "        print(\"messages: \", messages)\n",
    "    except Exception as e:\n",
    "        messages = [str(e)]\n",
    "    queue.put(messages)\n",
    "\n",
    "# Wrap AutoGen part into a function\n",
    "def chatbot_reply(input_text):\n",
    "    \"\"\"Chat with the agent through terminal.\"\"\"\n",
    "    queue = mp.Queue()\n",
    "    process = mp.Process(\n",
    "        target=initiate_chat,\n",
    "        args=(config_list, input_text, queue),\n",
    "    )\n",
    "    process.start()\n",
    "    try:\n",
    "        messages = queue.get(timeout=TIMEOUT)\n",
    "    except Exception as e:\n",
    "        messages = [str(e) if len(str(e)) > 0 else \"Invalid Request to OpenAI, please check your API keys.\"]\n",
    "    finally:\n",
    "        try:\n",
    "            process.terminate()\n",
    "        except:\n",
    "            pass\n",
    "    return messages\n",
    "\n",
    "...\n",
    "\n",
    "# Set up UI with Gradio\n",
    "with gr.Blocks() as demo:\n",
    "    ...\n",
    "    assistant, ragproxyagent = initialize_agents(config_list)\n",
    "\n",
    "    chatbot = gr.Chatbot(\n",
    "        [],\n",
    "        elem_id=\"chatbot\",\n",
    "        bubble_full_width=False,\n",
    "        avatar_images=(None, (os.path.join(os.path.dirname(__file__), \"autogen.png\"))),\n",
    "        # height=600,\n",
    "    )\n",
    "\n",
    "    txt_input = gr.Textbox(\n",
    "        scale=4,\n",
    "        show_label=False,\n",
    "        placeholder=\"Enter text and press enter\",\n",
    "        container=False,\n",
    "    )\n",
    "\n",
    "    with gr.Row():\n",
    "        txt_model = gr.Dropdown(\n",
    "            label=\"Model\",\n",
    "            choices=[\n",
    "                \"gpt-4\",\n",
    "                \"gpt-35-turbo\",\n",
    "                \"gpt-3.5-turbo\",\n",
    "            ],\n",
    "            allow_custom_value=True,\n",
    "            value=\"gpt-35-turbo\",\n",
    "            container=True,\n",
    "        )\n",
    "        txt_oai_key = gr.Textbox(\n",
    "            label=\"OpenAI API Key\",\n",
    "            placeholder=\"Enter key and press enter\",\n",
    "            max_lines=1,\n",
    "            show_label=True,\n",
    "            value=os.environ.get(\"OPENAI_API_KEY\", \"\"),\n",
    "            container=True,\n",
    "            type=\"password\",\n",
    "        )\n",
    "        ...\n",
    "\n",
    "    clear = gr.ClearButton([txt_input, chatbot])\n",
    "\n",
    "...\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ac860416-91ab-4630-b13e-7b2c83bde57f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3f4d8708-39ee-4d12-bbf0-eff4e81d1636",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_client=ollama.Client(host='http://localhost:11434')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "a362cd6c-7390-4371-91f8-d1ec088093fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"ceo\": \"李彦宏\", \"company\": \"百度公司\", \"market_value\": [\"市盈率 (倍)意向\", \"总市值 (亿元)\", \"自由现金流 (万元)\", \"营业收入 (亿元)\" ,\"每股收益 (元)\" ,\"市净率 (倍)意向\",\"总资产 (亿元)意向 \",\"净资产 (亿元)意向 \",\"流动资产 (亿元)意向 \",\"总负债 (亿元)意向 \",\"债务比率  (%)意向 \",\"流动比率  (%)意向 \",\"速动比率  (%)意向 \",\"股东权益比率 (%)意向 \",\"营业利润率  (%)意向 \",\"净利润率  (%)意向 \",\"存货周转率  (次/年)\" ,\"应收账款周转率  (次/年)\" ,\"流动资产周转率  (次/年)\" ,\"总资产周转率  (次/年)\" ,\"权益乘数  (倍)意向 \"]}\n",
      "company='百度公司' ceo='李彦宏' market_value=['市盈率 (倍)意向', '总市值 (亿元)', '自由现金流 (万元)', '营业收入 (亿元)', '每股收益 (元)', '市净率 (倍)意向', '总资产 (亿元)意向 ', '净资产 (亿元)意向 ', '流动资产 (亿元)意向 ', '总负债 (亿元)意向 ', '债务比率  (%)意向 ', '流动比率  (%)意向 ', '速动比率  (%)意向 ', '股东权益比率 (%)意向 ', '营业利润率  (%)意向 ', '净利润率  (%)意向 ', '存货周转率  (次/年)', '应收账款周转率  (次/年)', '流动资产周转率  (次/年)', '总资产周转率  (次/年)', '权益乘数  (倍)意向 ']\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class Country(BaseModel):\n",
    "  company: str\n",
    "  ceo: str\n",
    "  market_value: list[str]\n",
    "\n",
    "\n",
    "response = ollama_client.chat(\n",
    "  messages=[\n",
    "    {\n",
    "      'role': 'user',\n",
    "      'content': '介绍一下百度。',\n",
    "    }\n",
    "  ],\n",
    "  model='minicpm3:4b',\n",
    "  format=Country.model_json_schema(),\n",
    ")\n",
    "# response1 = ollama_client.chat(\n",
    "#   messages=[\n",
    "#     {\n",
    "#       'role': 'user',\n",
    "#       'content': '介绍一下百度。',\n",
    "#     }\n",
    "#   ],\n",
    "#   model='minicpm3:4b',\n",
    "#   # format=Country.model_json_schema(),\n",
    "# )\n",
    "country = Country.model_validate_json(response['message']['content'])\n",
    "print(response['message']['content'])\n",
    "# print(response1['message']['content'])\n",
    "print(country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e1f0576-f701-40ec-8e21-06ff3a5d3f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mem0 import Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdc00ade-3890-4dc5-90dd-c0d340f3bfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"llm\": {\n",
    "        \"provider\": \"ollama\",\n",
    "        \"config\": {\n",
    "            \"model\": \"minicpm3:4b\",\n",
    "            \"temperature\": 0,\n",
    "            \"max_tokens\": 8000,\n",
    "            \"ollama_base_url\": \"http://localhost:11434\",  # Ensure this URL is correct\n",
    "        },\n",
    "    },\n",
    "    \"embedder\": {\n",
    "        \"provider\": \"ollama\",\n",
    "        \"config\": {\n",
    "            \"model\": \"m3e:latest\",\n",
    "            # Alternatively, you can use \"snowflake-arctic-embed:latest\"\n",
    "            \"ollama_base_url\": \"http://localhost:11434\",\n",
    "        },\n",
    "    },\n",
    "    \"vector_store\": {\n",
    "        \"provider\": \"chroma\",\n",
    "        \"config\": {\n",
    "            \"collection_name\": \"test\",\n",
    "            \"path\": '/oper/ch/autogen/chromadb/chat_history',\n",
    "        }\n",
    "    },\n",
    "    \"version\": \"v1.1\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95164689-44bc-4033-ab00-b7bf33833a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory=Memory.from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91599de6-87cc-4237-a98d-d0d1f202bf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen import ConversableAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9ffd79-f04a-48f3-8f2d-454a644da931",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from autogen import ConversableAgent\n",
    "\n",
    "# Initialize Agent and Memory\n",
    "config_list=[{'model': 'llama3.2:3b-instruct-q8_0',\n",
    "  'base_url': 'http://localhost:11434/v1/',\n",
    "  'api_key': 'ollama'}]\n",
    "agent = ConversableAgent(\n",
    "    \"chatbot\",\n",
    "    llm_config={\"config_list\": config_list},\n",
    "    code_execution_config=False,\n",
    "    function_map=None,\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "# memory = MemoryClient(api_key=os.environ.get(\"MEM0_API_KEY\"))\n",
    "\n",
    "# Insert a conversation into memory\n",
    "conversation = [\n",
    "   {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Hi, I'm Best Buy's chatbot!\\n\\nThanks for being a My Best Buy TotalTM member.\\n\\nWhat can I help you with?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Seeing horizontal lines on our tv. TV model: Sony - 77\\\" Class BRAVIA XR A80K OLED 4K UHD Smart Google TV\"\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfc09563-47b1-411f-bda7-7b6104228966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'results': [{'id': '4cc44f5b-df7b-45e3-ab0f-76592b07b77c',\n",
       "   'memory': 'Seeing horizontal lines on TV',\n",
       "   'event': 'ADD'},\n",
       "  {'id': '2f7960f4-e2e1-4bcb-a8ce-59a53ba9426e',\n",
       "   'memory': 'TV model: Sony - 77\" Class BRAVIA XR A80K OLED 4K UHD Smart Google TV',\n",
       "   'event': 'ADD'}],\n",
       " 'relations': []}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.add(messages=conversation, user_id=\"customer_service_bot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f102c35-a8bd-452f-a5fd-f5ab48cd5943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'results': [{'id': '2f7960f4-e2e1-4bcb-a8ce-59a53ba9426e',\n",
       "   'memory': 'TV model: Sony - 77\" Class BRAVIA XR A80K OLED 4K UHD Smart Google TV',\n",
       "   'hash': '3008c013d78a7287b73df616280223c6',\n",
       "   'metadata': None,\n",
       "   'created_at': '2024-12-19T01:06:53.582016-08:00',\n",
       "   'updated_at': None,\n",
       "   'user_id': 'customer_service_bot'},\n",
       "  {'id': '4cc44f5b-df7b-45e3-ab0f-76592b07b77c',\n",
       "   'memory': 'Seeing horizontal lines on TV',\n",
       "   'hash': 'f1e908a95c1a6808afaf4c69db621f1a',\n",
       "   'metadata': None,\n",
       "   'created_at': '2024-12-19T01:06:53.378530-08:00',\n",
       "   'updated_at': None,\n",
       "   'user_id': 'customer_service_bot'}]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.get_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19582f9f-8786-4ad3-982c-89a4fe5f86b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '4cc44f5b-df7b-45e3-ab0f-76592b07b77c',\n",
       "  'memory': 'Seeing horizontal lines on TV',\n",
       "  'hash': 'f1e908a95c1a6808afaf4c69db621f1a',\n",
       "  'metadata': None,\n",
       "  'score': 396.0220326315845,\n",
       "  'created_at': '2024-12-19T01:06:53.378530-08:00',\n",
       "  'updated_at': None,\n",
       "  'user_id': 'customer_service_bot'},\n",
       " {'id': '2f7960f4-e2e1-4bcb-a8ce-59a53ba9426e',\n",
       "  'memory': 'TV model: Sony - 77\" Class BRAVIA XR A80K OLED 4K UHD Smart Google TV',\n",
       "  'hash': '3008c013d78a7287b73df616280223c6',\n",
       "  'metadata': None,\n",
       "  'score': 716.7570629729618,\n",
       "  'created_at': '2024-12-19T01:06:53.582016-08:00',\n",
       "  'updated_at': None,\n",
       "  'user_id': 'customer_service_bot'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_memories['results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f90266bf-dedc-4fd1-a835-7d5013116c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Number of requested results 100 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[autogen.oai.client: 12-19 17:31:18] {351} WARNING - Model llama3.2:3b-instruct-q8_0 is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:autogen.oai.client:Model llama3.2:3b-instruct-q8_0 is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reply : You're using a Sony 77\" Class BRAVIA XR A80K OLED 4K UHD Smart Google TV.\n"
     ]
    }
   ],
   "source": [
    "# Agent Inference\n",
    "data = \"Which TV am I using?\"\n",
    "\n",
    "relevant_memories = memory.search(data, user_id=\"customer_service_bot\")\n",
    "flatten_relevant_memories = \"\\n\".join([m[\"memory\"] for m in relevant_memories['results']])\n",
    "\n",
    "prompt = f\"\"\"Answer the user question considering the memories.\n",
    "Memories:\n",
    "{flatten_relevant_memories}\n",
    "\\n\\n\n",
    "Question: {data}\n",
    "\"\"\"\n",
    "\n",
    "reply = agent.generate_reply(messages=[{\"content\": prompt, \"role\": \"user\"}])\n",
    "print(\"Reply :\", reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9308500-296a-4e58-9588-ef6425a28c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi Agent Conversation\n",
    "manager = ConversableAgent(\n",
    "    \"manager\",\n",
    "    system_message=\"You are a manager who helps in resolving customer issues.\",\n",
    "    llm_config={\"config_list\": [{\"model\": \"gpt-4\", \"temperature\": 0, \"api_key\": os.environ.get(\"OPENAI_API_KEY\")}]},\n",
    "    human_input_mode=\"NEVER\"\n",
    ")\n",
    "\n",
    "customer_bot = ConversableAgent(\n",
    "    \"customer_bot\",\n",
    "    system_message=\"You are a customer service bot who gathers information on issues customers are facing.\",\n",
    "    llm_config={\"config_list\": [{\"model\": \"gpt-4\", \"temperature\": 0, \"api_key\": os.environ.get(\"OPENAI_API_KEY\")}]},\n",
    "    human_input_mode=\"NEVER\"\n",
    ")\n",
    "\n",
    "data = \"What appointment is booked?\"\n",
    "\n",
    "relevant_memories = memory.search(data, user_id=\"customer_service_bot\")\n",
    "flatten_relevant_memories = \"\\n\".join([m[\"memory\"] for m in relevant_memories])\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Context:\n",
    "{flatten_relevant_memories}\n",
    "\\n\\n\n",
    "Question: {data}\n",
    "\"\"\"\n",
    "\n",
    "result = manager.send(prompt, customer_bot, request_reply=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
